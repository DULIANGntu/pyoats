{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628e8b40-08c5-49a7-9048-1108cb23b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624b4a7f-d852-4cbe-abda-696381afaf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd21ec7-7f01-451c-8875-c0ed6411218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view   \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ac4e33-81a1-4792-9705-504ad4867cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.generator.univariate import UnivariateDataGenerator\n",
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8ad3c7-97d8-47a1-bcae-e3aac46138d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c521f5-f82a-420f-af7a-979c163caf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 40,10\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93de1f-476a-488b-8dd8-9f684364dd57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Generating Univariate Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468a247-9eca-426b-be62-386bea10d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UnivariateDataGenerator(stream_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf757-d9b9-4e0d-a353-46b76ad86c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.collective_seasonal_outliers(0.1, 1., 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073555c-7f12-4498-91b2-a89904e32b3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bbafc-c853-43a7-b125-33ddf3da7d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c1a18-9035-49f3-ae86-86b1c75d22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generator.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e13092-70d0-4740-a991-ccd8965cefb5",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd75a0d-2a5a-4cfb-aad7-36e6366c2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2)\n",
    "\n",
    "axes[0].plot(generator.test)\n",
    "axes[1].plot(generator.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f71479-5cd0-4c9b-acb6-0dc2b754c67a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e345e-0eae-4fb7-8aa7-52d169148eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./data/univar-synth/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f282864-d266-4c1f-b8bd-6b73e011a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Global\n",
    "out_type = \"point_global\"\n",
    "config_1 = [0.05, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.point_global_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eee61-0ea2-4913-a446-9bcd3540b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Contextual\n",
    "out_type = \"point_contextual\"\n",
    "config_1 = [0.05, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.point_contextual_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34ee3c-8adc-4b69-9bad-334aa43779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Global\n",
    "out_type = \"collective_global\"\n",
    "config_1 = [0.05, 50, 1.1] #ratio, radius, coef\n",
    "config_2 = [0.05, 50, 1.25] #ratio, radius, coef\n",
    "config_3 = [0.05, 50, 1.5] #ratio, radius, coef\n",
    "config_4 = [0.05, 50, 2] #ratio, radius, coef\n",
    "config_5 = [0.05, 50, 3] #ratio, radius, coef\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    *args, coef = config\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_global_outliers(*args, \"square\", coef=coef)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb5ddd-9060-4310-b6c9-c05b5ba053d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Trend\n",
    "out_type = \"collective_trend\"\n",
    "config_1 = [0.05, 0.01, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 0.02, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 0.03, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 0.04, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 0.05, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_trend_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784f76b-664e-45af-8ed6-4a0b51f2395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Seasonal\n",
    "out_type = \"collective_seasonal\"\n",
    "config_1 = [0.1, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.1, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.1, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.1, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.1, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_seasonal_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c18388-89fa-4954-a693-e7d2ea535038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03888d-fb73-460d-b742-0b5c1131ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH0 = \"./data/univar-synth/point_global/\"\n",
    "PATH1 = \"./data/univar-synth/point_contextual/\"\n",
    "PATH2 = \"./data/univar-synth/collective_global/\"\n",
    "PATH3 = \"./data/univar-synth/collective_trend/\"\n",
    "PATH4 = \"./data/univar-synth/collective_seasonal/\"\n",
    "PATHS = [PATH0, PATH1, PATH2, PATH3, PATH4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaef81-87c2-4d5d-949b-c50e6047c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    \n",
    "    for f in file_list:\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2)\n",
    "        axes[0].set_title(f)\n",
    "        axes[0].plot(test)\n",
    "        axes[1].plot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24463bb6-4cc9-48f4-9b92-5781b967cc1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a412f15e-5dd6-404d-b780-1c33747fc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCounter:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "    def process(self, preds, labels):\n",
    "        preds = preds.copy()\n",
    "        labels = labels.copy()\n",
    "        ground_truth_ones = np.where(labels == 1)[0]\n",
    "        pred_ones = np.where(preds == 1)[0]\n",
    "        \n",
    "        ranges = self._consecutive(ground_truth_ones)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for r in ranges:\n",
    "            intersect = np.intersect1d(r, pred_ones, assume_unique=True)\n",
    "            if intersect.size != 0:\n",
    "                tp += r.size\n",
    "                preds[intersect] = 0\n",
    "                pred_ones = np.where(preds == 1)[0]\n",
    "            else:\n",
    "                fn += r.size\n",
    "            \n",
    "        fp += pred_ones.size\n",
    "        tn += preds.size - tp - fp - fn\n",
    "        \n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.tn += tn\n",
    "        self.fn += fn\n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _consecutive(self, data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self.tp/(self.fn+self.tp)\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self.fp/(self.tn+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def tnr(self):\n",
    "        return self.tn/(self.tn+self.fp)\n",
    "        \n",
    "    @property\n",
    "    def fnr(self):\n",
    "        return self.fn/(self.fn+self.tp)\n",
    "        \n",
    "    @property\n",
    "    def precision(self):\n",
    "        return self.tp/(self.tp+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def recall(self):\n",
    "        return self.tp/(self.tp+self.fn)\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        return (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e7322-4b76-4857-9919-230823581a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6f44f-5f34-476b-9fe6-532389fb7b8e",
   "metadata": {},
   "source": [
    "## Metric 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc98de-a929-4f69-a5e8-ea5df3af6e75",
   "metadata": {},
   "source": [
    "### -- Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da67fb1-f805-446d-9603-5a48486389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH0 = \"../data/univar-synth/point_global/\"\n",
    "PATH1 = \"../data/univar-synth/point_contextual/\"\n",
    "PATH2 = \"../data/univar-synth/collective_global/\"\n",
    "PATH3 = \"../data/univar-synth/collective_trend/\"\n",
    "PATH4 = \"../data/univar-synth/collective_seasonal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f027dc8-57c7-4121-b46d-7a083c9a7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [PATH0, PATH1, PATH2, PATH3, PATH4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5821ed9e-7c6f-4c78-9491-6164955eadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/univar-synth/unsup1tuned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943d9f31-1c51-40dc-ad9d-2ce07c69edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_wo_label(data, preds):\n",
    "    \"\"\"\n",
    "    Returns a few quantitative metrics for us to use for evaluation when labels\n",
    "    are not provided.\n",
    "    Parameters\n",
    "    -----------\n",
    "    df: pd.Dataframe\n",
    "        the dataframe with 'timestamp', 'value' and 'predict' columns\n",
    "        where 'predict' is 1 for those predicted as anomalous and 0 otherwise.\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple\n",
    "        (number of anomalies,\n",
    "        % of anomnalies,\n",
    "        avg. distance between mean and all anomalies (yaxis),\n",
    "        avg. time distance between consecutive anomalies,\n",
    "        avg. cycle distance between consecutive anomalies,\n",
    "        maximum range between non anomaly points (yaxis)\n",
    "        )\n",
    "    \"\"\"\n",
    "    num_anomalies = preds.sum()\n",
    "    percent_anomalies = num_anomalies/len(preds)\n",
    "\n",
    "    mean_val = data.mean(axis=0)\n",
    "\n",
    "    pred_anomalies = data[preds == 1]\n",
    "    pred_non_anomalies = data[preds == 0]\n",
    "\n",
    "    avg_anom_dist_from_mean_val = np.linalg.norm(pred_anomalies - mean_val, axis=-1).mean()\n",
    "    avg_cycles_delta_between_anomalies = np.diff(np.where(preds==1)[0]).mean()\n",
    "    try:\n",
    "        max_range_non_anomalies = (np.abs(pred_non_anomalies.max() - pred_non_anomalies.min())).mean() \n",
    "    except ValueError: max_range_non_anomalies = np.nan\n",
    "\n",
    "    return (num_anomalies,\n",
    "            percent_anomalies,\n",
    "            avg_anom_dist_from_mean_val,\n",
    "            avg_cycles_delta_between_anomalies,\n",
    "            max_range_non_anomalies)\n",
    "\n",
    "\n",
    "def compute_objective(data, preds):\n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    (num_anomalies, percent_anomalies, avg_anom_dist_from_mean_val,\n",
    "    avg_cycles_delta_between_anomalies, max_range_non_anomalies) = _eval_wo_label(data, preds)\n",
    "\n",
    "    obj = 1e4 * percent_anomalies + max_range_non_anomalies - avg_cycles_delta_between_anomalies\n",
    "    \n",
    "    # If nan, should return number in case it always gives nan\n",
    "    if np.isnan(obj) or np.isinf(obj):\n",
    "        obj = 1e10\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace353be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcb180",
   "metadata": {},
   "source": [
    "### Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070298a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantile Model\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 100, 1000)\n",
    "            threshold = trial.suggest_float(\"threshold\", 0.95, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            model = QuantileModel(window)\n",
    "            scores = model.get_scores(test_extend)[window:] \n",
    "            \n",
    "            return compute_objective(test, scores)\n",
    "        \n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        threshold = study.best_params[\"threshold\"]\n",
    "        model = QuantileModel(window, threshold)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        \n",
    "        scores = model.get_scores(test_extend)[window:] \n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"quantile/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"quantile/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(scores, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66ecbf",
   "metadata": {},
   "source": [
    "### MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295cf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MA Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            model = MovingAverageModel(window)\n",
    "            scores = np.abs(model.get_scores(test_extend)[window:])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            return compute_objective(test, preds)\n",
    "           \n",
    "       \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=150)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        model = MovingAverageModel(window)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        \n",
    "        scores = np.abs(model.get_scores(test_extend)[window:] )\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"ma/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"ma/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acc764",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            p = trial.suggest_int(\"p\", 1, 20)\n",
    "            d = trial.suggest_int(\"d\", 0, 3)\n",
    "            q = trial.suggest_int(\"q\", 0, 20)\n",
    "            q_risk = trial.suggest_float(\"q_risk\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    " \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = ARIMAModel(p, d, q)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend))\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q_risk, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            s.process(preds, labels)\n",
    "        \n",
    "            if s.tp == 0 and s.fp == 0: return -1\n",
    "            if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "            if s.precision == 0 and s.recall == 0: return -1\n",
    "            if np.isnan(s.f1): return -1\n",
    "            return s.f1\n",
    " \n",
    "       \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=20)\n",
    "       \n",
    "        p = study.best_params[\"p\"]\n",
    "        d = study.best_params[\"d\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        q_risk = study.best_params[\"q_risk\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "        model = ARIMAModel(p, d, q)\n",
    "        model.fit(train)\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend))\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q_risk, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"arima/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"arima/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95cdb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439bb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IForest Model \n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        \n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "                \n",
    "            model = IsolationForestModel()\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test))\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            \n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            s.process(preds, labels)\n",
    "        \n",
    "            if s.tp == 0 and s.fp == 0: return -1\n",
    "            if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "            if s.precision == 0 and s.recall == 0: return -1\n",
    "            if np.isnan(s.f1): return -1\n",
    "            return s.f1\n",
    " \n",
    "            \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=150)\n",
    "       \n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        model = IsolationForestModel()\n",
    "        model.fit(train)\n",
    "        scores = np.abs(model.get_scores(test))\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"iforest/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"iforest/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-scores.txt\", scores)\n",
    "        np.savetxt(save+\"-preds.txt\", preds)\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765ec1b",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08b776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regression Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            lags = trial.suggest_int(\"lags\", 1, 5)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    " \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = RegressionModel(window, n_steps, lags)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            return compute_objective(test, preds)\n",
    "      \n",
    "    \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        lags = study.best_params[\"lags\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "        model = RegressionModel(window,n_steps, lags)\n",
    "        model.fit(train)\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend)[0])\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"regression/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"regression/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-scores.txt\", scores)\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef4025",
   "metadata": {},
   "source": [
    "### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8866c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# NBEATSModel\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "\n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = NBEATSModel(window, n_steps, use_gpu=True)            \n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            return compute_objective(test, preds)\n",
    " \n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"nbeats/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"nbeats/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"nbeats/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be305c",
   "metadata": {},
   "source": [
    "### NHiTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d803257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            return compute_objective(test, preds)\n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"nhits/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"nhits/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"nhits/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85e191",
   "metadata": {},
   "source": [
    "### RNN(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055223b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#supress output\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "\n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = RNNModel(window, n_steps, rnn_model=\"GRU\")\n",
    "            \n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            return compute_objective(test, preds)\n",
    " \n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = RNNModel(window, n_steps, use_gpu=True, rnn_model=\"GRU\")\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"rnn_gru/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"rnn_gru/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"rnn_gru/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b4567",
   "metadata": {},
   "source": [
    "### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173ab58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "               \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            \n",
    "            model = TCNModel(window, n_steps, use_gpu=True)\n",
    "            #model.params = params\n",
    "            #model._init_model(**model.params)\n",
    "            \n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            return compute_objective(test, preds)\n",
    " \n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = TCNModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"tcn/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"tcn/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"tcn/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30be85",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc5af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = TransformerModel(window, n_steps, use_gpu=True)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            return compute_objective(test, preds)\n",
    "        \n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = TransformerModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"transformer/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"transformer/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"transformer/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40343d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in PATHS:\n",
    "    scorer = ScoreCounter()\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    for f in file_list:\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        preds = np.loadtxt(SAVE_DIR+\"transformer/\"+f+\"-preds.txt\")\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd4af4-233d-4793-94ff-bb7e5689474d",
   "metadata": {},
   "source": [
    "## Jenks Natural Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f1dab5b-48b1-44e9-abad-7cc92da3497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347, 137, 18891, 625, 0.35699588477366256, 0.007199915913390792, 0.9928000840866092, 0.6430041152263375, 0.7169421487603306, 0.35699588477366256, 0.4766483516483517\n",
      "46, 410, 18611, 933, 0.04698672114402452, 0.021555123284790496, 0.9784448767152095, 0.9530132788559755, 0.10087719298245613, 0.04698672114402452, 0.06411149825783972\n",
      "400, 341, 18659, 600, 0.4, 0.017947368421052632, 0.9820526315789474, 0.6, 0.5398110661268556, 0.4, 0.4595060310166571\n",
      "604, 516, 18580, 300, 0.668141592920354, 0.02702136573104315, 0.9729786342689568, 0.33185840707964603, 0.5392857142857143, 0.668141592920354, 0.5968379446640316\n",
      "1771, 501, 17628, 100, 0.9465526456440406, 0.027635280489822937, 0.9723647195101771, 0.05344735435595938, 0.7794894366197183, 0.9465526456440406, 0.8549360366883899\n"
     ]
    }
   ],
   "source": [
    "model = \"quantile\"\n",
    "for path in PATHS:\n",
    "    scorer = ScoreCounter()\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    for f in file_list:\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        scores = np.loadtxt(SAVE_DIR+f\"{model}/\"+f+\"-scores.txt\")\n",
    "        \n",
    "        thres = jenkspy.jenks_breaks(scores, nb_class=20)[-2]\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0d34c-c9e3-48ec-b81b-38ce82bf1054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb552ba-9f01-4a6d-9020-fd702fea1764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb81935-82ba-4abd-b68a-5426bbe8147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f294cf-c07a-450c-adbc-7080b47293d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ed823-b266-44de-a41a-fb99a52a1396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301c051f-6787-4d28-baea-d1849b63a651",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52c6679-4a62-4f35-b44e-65a9f8a6358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7, 0, 19028, 965, 0.00720164609053498, 0.0, 1.0, 0.992798353909465, 1.0, 0.00720164609053498, 0.014300306435137895\n",
      "7, 32, 18989, 972, 0.007150153217568948, 0.0016823510856421849, 0.9983176489143578, 0.992849846782431, 0.1794871794871795, 0.007150153217568948, 0.0137524557956778\n",
      "200, 6, 18994, 800, 0.2, 0.00031578947368421053, 0.9996842105263158, 0.8, 0.970873786407767, 0.2, 0.3316749585406302\n",
      "304, 8, 19088, 600, 0.336283185840708, 0.00041893590280687055, 0.9995810640971932, 0.6637168141592921, 0.9743589743589743, 0.336283185840708, 0.5000000000000001\n",
      "269, 6, 18123, 1602, 0.14377338321753072, 0.00033096144299189144, 0.9996690385570081, 0.8562266167824693, 0.9781818181818182, 0.14377338321753072, 0.25069897483690584\n"
     ]
    }
   ],
   "source": [
    "model = \"transformer\"\n",
    "for path in PATHS:\n",
    "    scorer = ScoreCounter()\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    for f in file_list:\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        preds = np.loadtxt(SAVE_DIR+f\"{model}/\"+f+\"-preds.txt\")\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d810f46-7a04-4280-866c-4b4e7b680309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
