{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628e8b40-08c5-49a7-9048-1108cb23b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd21ec7-7f01-451c-8875-c0ed6411218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view   \n",
    "import optuna\n",
    "import jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ac4e33-81a1-4792-9705-504ad4867cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.generator.univariate import UnivariateDataGenerator\n",
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8ad3c7-97d8-47a1-bcae-e3aac46138d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c521f5-f82a-420f-af7a-979c163caf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 40,10\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93de1f-476a-488b-8dd8-9f684364dd57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Generating Univariate Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468a247-9eca-426b-be62-386bea10d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UnivariateDataGenerator(stream_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf757-d9b9-4e0d-a353-46b76ad86c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.collective_seasonal_outliers(0.1, 1., 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073555c-7f12-4498-91b2-a89904e32b3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bbafc-c853-43a7-b125-33ddf3da7d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c1a18-9035-49f3-ae86-86b1c75d22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generator.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e13092-70d0-4740-a991-ccd8965cefb5",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd75a0d-2a5a-4cfb-aad7-36e6366c2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2)\n",
    "\n",
    "axes[0].plot(generator.test)\n",
    "axes[1].plot(generator.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f71479-5cd0-4c9b-acb6-0dc2b754c67a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e345e-0eae-4fb7-8aa7-52d169148eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./data/univar-synth/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f282864-d266-4c1f-b8bd-6b73e011a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Global\n",
    "out_type = \"point_global\"\n",
    "config_1 = [0.05, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.point_global_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eee61-0ea2-4913-a446-9bcd3540b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Contextual\n",
    "out_type = \"point_contextual\"\n",
    "config_1 = [0.05, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.point_contextual_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34ee3c-8adc-4b69-9bad-334aa43779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Global\n",
    "out_type = \"collective_global\"\n",
    "config_1 = [0.05, 50, 1.1] #ratio, radius, coef\n",
    "config_2 = [0.05, 50, 1.25] #ratio, radius, coef\n",
    "config_3 = [0.05, 50, 1.5] #ratio, radius, coef\n",
    "config_4 = [0.05, 50, 2] #ratio, radius, coef\n",
    "config_5 = [0.05, 50, 3] #ratio, radius, coef\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    *args, coef = config\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_global_outliers(*args, \"square\", coef=coef)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{coef}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb5ddd-9060-4310-b6c9-c05b5ba053d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Trend\n",
    "out_type = \"collective_trend\"\n",
    "config_1 = [0.05, 0.01, 50] #ratio, factor, radius\n",
    "config_2 = [0.05, 0.02, 50] #ratio, factor, radius\n",
    "config_3 = [0.05, 0.03, 50] #ratio, factor, radius\n",
    "config_4 = [0.05, 0.04, 50] #ratio, factor, radius\n",
    "config_5 = [0.05, 0.05, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_trend_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784f76b-664e-45af-8ed6-4a0b51f2395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collective Seasonal\n",
    "out_type = \"collective_seasonal\"\n",
    "config_1 = [0.1, 1.1, 50] #ratio, factor, radius\n",
    "config_2 = [0.1, 1.25, 50] #ratio, factor, radius\n",
    "config_3 = [0.1, 1.5, 50] #ratio, factor, radius\n",
    "config_4 = [0.1, 2, 50] #ratio, factor, radius\n",
    "config_5 = [0.1, 3, 50] #ratio, factor, radius\n",
    "\n",
    "for idx, config in enumerate([config_1, config_2, config_3, config_4, config_5]):\n",
    "    generator = UnivariateDataGenerator(stream_length=5000)\n",
    "    generator.collective_seasonal_outliers(*config)\n",
    "    \n",
    "    # save train\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-train.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.train)\n",
    "    \n",
    "    # save test\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-test.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.test)\n",
    "    \n",
    "    # save labels\n",
    "    file_name = f\"{out_type}/{idx}-{out_type}-factor{config[1]}-labels.txt\"\n",
    "    np.savetxt(SAVE_DIR+file_name, generator.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c18388-89fa-4954-a693-e7d2ea535038",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03888d-fb73-460d-b742-0b5c1131ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH0 = \"./data/univar-synth/point_global/\"\n",
    "PATH1 = \"./data/univar-synth/point_contextual/\"\n",
    "PATH2 = \"./data/univar-synth/collective_global/\"\n",
    "PATH3 = \"./data/univar-synth/collective_trend/\"\n",
    "PATH4 = \"./data/univar-synth/collective_seasonal/\"\n",
    "PATHS = [PATH0, PATH1, PATH2, PATH3, PATH4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaef81-87c2-4d5d-949b-c50e6047c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    \n",
    "    for f in file_list:\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2)\n",
    "        axes[0].set_title(f)\n",
    "        axes[0].plot(test)\n",
    "        axes[1].plot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24463bb6-4cc9-48f4-9b92-5781b967cc1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a412f15e-5dd6-404d-b780-1c33747fc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCounter:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "    def process(self, preds, labels):\n",
    "        preds = preds.copy()\n",
    "        labels = labels.copy()\n",
    "        ground_truth_ones = np.where(labels == 1)[0]\n",
    "        pred_ones = np.where(preds == 1)[0]\n",
    "        \n",
    "        ranges = self._consecutive(ground_truth_ones)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for r in ranges:\n",
    "            intersect = np.intersect1d(r, pred_ones, assume_unique=True)\n",
    "            if intersect.size != 0:\n",
    "                tp += r.size\n",
    "                preds[intersect] = 0\n",
    "                pred_ones = np.where(preds == 1)[0]\n",
    "            else:\n",
    "                fn += r.size\n",
    "            \n",
    "        fp += pred_ones.size\n",
    "        tn += preds.size - tp - fp - fn\n",
    "        \n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.tn += tn\n",
    "        self.fn += fn\n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _consecutive(self, data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self.tp/(self.fn+self.tp)\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self.fp/(self.tn+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def tnr(self):\n",
    "        return self.tn/(self.tn+self.fp)\n",
    "        \n",
    "    @property\n",
    "    def fnr(self):\n",
    "        return self.fn/(self.fn+self.tp)\n",
    "        \n",
    "    @property\n",
    "    def precision(self):\n",
    "        return self.tp/(self.tp+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def recall(self):\n",
    "        return self.tp/(self.tp+self.fn)\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        return (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e7322-4b76-4857-9919-230823581a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6f44f-5f34-476b-9fe6-532389fb7b8e",
   "metadata": {},
   "source": [
    "## Metric 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc98de-a929-4f69-a5e8-ea5df3af6e75",
   "metadata": {},
   "source": [
    "### -- Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da67fb1-f805-446d-9603-5a48486389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH0 = \"../data/univar-synth/point_global/\"\n",
    "PATH1 = \"../data/univar-synth/point_contextual/\"\n",
    "PATH2 = \"../data/univar-synth/collective_global/\"\n",
    "PATH3 = \"../data/univar-synth/collective_trend/\"\n",
    "PATH4 = \"../data/univar-synth/collective_seasonal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f027dc8-57c7-4121-b46d-7a083c9a7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [PATH0, PATH1, PATH2, PATH3, PATH4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5821ed9e-7c6f-4c78-9491-6164955eadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/univar-synth/unsup1tuned-metric2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "943d9f31-1c51-40dc-ad9d-2ce07c69edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNSUPERVISED LEARNING METRICS\n",
    "Author:     Bob Stienen\n",
    "License:    MIT License\n",
    "Source:     http://www.github.com/bstienen/AUMVC\n",
    "\n",
    "Implementation of the Area under the Mass-Volume Curve algorithm as by\n",
    "- Stephan Clémençon and Jeremie Jakubowicz, Scoring anomalies: a M-estimation\n",
    "  formulation approach. 2013-04\n",
    "\n",
    "Implementation is inspired by\n",
    "   https://github.com/albertcthomas/anomaly_tuning\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def aumvc(scoring_function,\n",
    "          X_test,\n",
    "          N_mc=100000,\n",
    "          N_levelsets=100,\n",
    "          normalise=True):\n",
    "    \"\"\" Calculate the area under the mass-volume curve for an anomaly detection\n",
    "    function or algorithm\n",
    "\n",
    "    This function uses monte carlo sampling in the parameter space box spanned\n",
    "    by the provided test data in order to estimate the level set of the\n",
    "    scoring function. For higher dimensionalities the amount of sampled data\n",
    "    points would yield this algorithm intractable. In these cases the use of\n",
    "    the `aumvc_hd` function is advised instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scoring_function: function\n",
    "        Function that takes datapoints as numpy.ndarray (nPoints, nFeatures)\n",
    "        and returns an anomaly score. This score should be in range [0,1],\n",
    "        where 1 indicates the point not being an anomaly (and 0 that the point\n",
    "        *is* an anomaly).\n",
    "    X_test: numpy.ndarray of shape (nPoints, nFeatures)\n",
    "        Datapoints used for testing the algorithm.\n",
    "    N_mc: int (default: 100,000)\n",
    "        Number of datapoints to sample in the parameter space to estimate the\n",
    "        level sets of the scoring function.\n",
    "    N_levelsets: int (default: 100)\n",
    "        Number of level sets to evaluate.\n",
    "    normalise: bool (default: True)\n",
    "        Indicates if output scores of the scoring_function should be normalised\n",
    "        before calculating the mass-volume curve. \"\"\"\n",
    "\n",
    "    # Get ranges for the test data\n",
    "    mins = np.amin(X_test, axis=0)\n",
    "    maxs = np.amax(X_test, axis=0)\n",
    "    \n",
    "    if X_test.ndim==1:\n",
    "        mins = np.array([mins])\n",
    "        maxs = np.array([maxs])\n",
    "       \n",
    "\n",
    "    # Generate uniform MC data\n",
    "    U = np.random.rand(N_mc, len(mins))*(maxs-mins)+mins\n",
    "\n",
    "    # Calculate volume of total cube\n",
    "    vol_tot_cube = np.prod(maxs - mins)\n",
    "\n",
    "    # Score test and MC data\n",
    "    score_U = scoring_function(U)\n",
    "    score_test = scoring_function(X_test)\n",
    "\n",
    "    # Do normalising if needed\n",
    "    if normalise:\n",
    "        minimum = min(np.amin(score_U), np.amin(score_test))\n",
    "        maximum = max(np.amax(score_U), np.amax(score_test))\n",
    "        score_U = (score_U - minimum) / (maximum - minimum)\n",
    "        score_test = (score_test - minimum) / (maximum - minimum)\n",
    "\n",
    "    # Calculate alphas to use\n",
    "    alphas = np.linspace(0, 1, N_levelsets)\n",
    "\n",
    "    # Compute offsets\n",
    "    offsets = np.percentile(score_test, 100 * (1 - alphas))\n",
    "\n",
    "    # Compute volumes of associated level sets\n",
    "    volumes = (np.array([np.mean(score_U >= offset)\n",
    "                        for offset in offsets]) * vol_tot_cube)\n",
    "\n",
    "    # Calculating area under the curve\n",
    "    area = auc(alphas, volumes)\n",
    "\n",
    "    # Return area and curve variables\n",
    "    return (area, alphas, volumes)\n",
    "\n",
    "\n",
    "def aumvc_hd(scoring_function_generator,\n",
    "             X_train,\n",
    "             X_test,\n",
    "             N_selected_dim=5,\n",
    "             N_iterations=100,\n",
    "             N_mc=100000,\n",
    "             N_levelsets=1000,\n",
    "             normalise=True):\n",
    "    \"\"\" Calculate the area under the mass-volume curve for an anomaly detection\n",
    "    function or algorithm working in high-dimensional parameter spaces\n",
    "\n",
    "    The curse of dimensionality is avoided by taking the average over multiple\n",
    "    AUMVC values for randomly selected subspaces of the parameter space under\n",
    "    consideration. The AUMVCs are calculated using the `aumvc` function above.\n",
    "    As this requires a retraining of the scoring function for each random\n",
    "    subspace, the `aumvc_hd` function does not take a scoring function as\n",
    "    input, but rather a generator of scoring functions. This function should\n",
    "    take the training data as input and return a scoring function (see\n",
    "    description of `aumvc` for requirements of this function).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scoring_function_generator: function\n",
    "        Function that takes training datapoints as numpy.ndarray of shape\n",
    "        (nPoints, nFeatures) and returns a scoring function. See description of\n",
    "        `aumvc` function for requirements on the scoring function.\n",
    "    X_train: numpy.ndarray of shape (nPoints, nFeatures)\n",
    "        Data points for which randomly selected subspaces are passed to the\n",
    "        scoring function generator for creation of the scoring function.\n",
    "    X_test: numpy.ndarray of shape (nPoints, nFeatures)\n",
    "        Data points used for testing the algorithm. Number of data points does\n",
    "        not have to match the number of training points, but the number of\n",
    "        features *does* have to match.\n",
    "    N_selected_dim: int (default=5)\n",
    "        Number of dimensions selected for the random subspace generation. This\n",
    "        number should be equal to or smaller than the number of features in\n",
    "        the testing data.\n",
    "    N_iterations: int (default=100)\n",
    "        Number of random subspaces have to be evaluated. A warning will be\n",
    "        raised if this number is higher than the total number of unique\n",
    "        combinations that can be randomly selected from the provided parameter\n",
    "        space.\n",
    "    N_mc: int (default=100,000)\n",
    "        Number of datapoints to sample in the parameter space to estimate the\n",
    "        level sets of the scoring function.\n",
    "    N_levelsets: int (default=100)\n",
    "        Number of level sets to evaluate.\n",
    "    normalise: bool (default: True)\n",
    "        Indicates if output scores of the scoring_function should be normalised\n",
    "        before calculating the mass-volume curve. \"\"\"\n",
    "\n",
    "    # Check if N_selected_dim <= dim(X_test)\n",
    "    data_dim = X_test.shape[1]\n",
    "    if data_dim > N_selected_dim:\n",
    "        raise Exception(\"\"\"The number of dimensions to select in each iteration\n",
    "is larger than the number of dimensions in the provided data.\"\"\")\n",
    "\n",
    "    # Check if the dimensionality of training data matches the dimensionality\n",
    "    # of the testing data\n",
    "    if X_train.shape[1] != data_dim:\n",
    "        raise Exception(\"\"\"The number of features in the training data does not\n",
    "match the number of features in the testing data.\"\"\")\n",
    "\n",
    "    # Check if the number of unique random subspaces is significantly larger\n",
    "    # (i.e. > a factor of 2) than the requested number of iterations\n",
    "    N_unique = np.random.choice(data_dim, N_selected_dim, replace=False)\n",
    "    if N_unique < 2 * N_selected_dim:\n",
    "        warnings.warn(\"\"\"The number of unique combinations of the dimensions of\n",
    "the input space is smaller than the number of dimensions to select in each\n",
    "iterations.\"\"\")\n",
    "\n",
    "    # Initialise final AUMVC variable\n",
    "    area_hd = 0\n",
    "\n",
    "    # Run over each iteration\n",
    "    for _ in range(N_iterations):\n",
    "\n",
    "        # Make feature subselection\n",
    "        features = np.random.choice(data_dim, N_selected_dim, replace=False)\n",
    "        X_selection = X_test[:, features]\n",
    "        X_train_selection = X_train[:, features]\n",
    "\n",
    "        # Train scoring function\n",
    "        scoring_function = scoring_function_generator(X_train_selection)\n",
    "\n",
    "        # Calculate area under curve and collect it in final variable\n",
    "        area, _, _ = aumvc(scoring_function,\n",
    "                           X_selection,\n",
    "                           N_mc,\n",
    "                           N_levelsets,\n",
    "                           normalise)\n",
    "        area_hd += area\n",
    "\n",
    "    # Return mean area\n",
    "    return area_hd / N_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace353be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcb180",
   "metadata": {},
   "source": [
    "### Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070298a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457, 384, 18644, 515, 0.47016460905349794, 0.02018078620979609, 0.9798192137902039, 0.529835390946502, 0.5434007134363853, 0.47016460905349794, 0.5041367898510756\n"
     ]
    }
   ],
   "source": [
    "# Quantile Model\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 100, 1000)\n",
    "            threshold = trial.suggest_float(\"threshold\", 0.95, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            model = QuantileModel(window)\n",
    "            scores = model.get_scores(test_extend)[window:] \n",
    "            \n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x.flatten()), test)\n",
    "            return auc\n",
    "        \n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        threshold = study.best_params[\"threshold\"]\n",
    "        model = QuantileModel(window, threshold)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        \n",
    "        scores = model.get_scores(test_extend)[window:] \n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"quantile/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"quantile/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(scores, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66ecbf",
   "metadata": {},
   "source": [
    "### MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c295cf97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606, 86, 18942, 366, 0.6234567901234568, 0.0045196552449022495, 0.9954803447550977, 0.3765432098765432, 0.8757225433526011, 0.6234567901234568, 0.7283653846153846\n",
      "139, 2101, 16920, 840, 0.14198161389172625, 0.1104568634666947, 0.8895431365333053, 0.8580183861082737, 0.06205357142857143, 0.14198161389172625, 0.08636222429325878\n",
      "600, 1475, 17525, 400, 0.6, 0.07763157894736843, 0.9223684210526316, 0.4, 0.2891566265060241, 0.6, 0.3902439024390244\n",
      "704, 358, 18738, 200, 0.7787610619469026, 0.018747381650607457, 0.9812526183493926, 0.22123893805309736, 0.6629001883239172, 0.7787610619469026, 0.7161749745676501\n",
      "1871, 2142, 15987, 0, 1.0, 0.11815323514810525, 0.8818467648518947, 0.0, 0.46623473710441066, 1.0, 0.6359619306594153\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# MA Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            model = MovingAverageModel(window)\n",
    "            scores = np.abs(model.get_scores(test_extend)[window:])\n",
    "            \n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x.flatten()), test)\n",
    "            return auc\n",
    "           \n",
    "       \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=150)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        \n",
    "        model = MovingAverageModel(window)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        \n",
    "        scores = np.abs(model.get_scores(test_extend)[window:] )\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = jenkspy.jenks_breaks(scores, nb_class=20)[-2]\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"ma/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"ma/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acc764",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            p = trial.suggest_int(\"p\", 1, 20)\n",
    "            d = trial.suggest_int(\"d\", 0, 3)\n",
    "            q = trial.suggest_int(\"q\", 0, 20)\n",
    "            q_risk = trial.suggest_float(\"q_risk\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    " \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = ARIMAModel(p, d, q)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend))\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q_risk, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            s.process(preds, labels)\n",
    "        \n",
    "            if s.tp == 0 and s.fp == 0: return -1\n",
    "            if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "            if s.precision == 0 and s.recall == 0: return -1\n",
    "            if np.isnan(s.f1): return -1\n",
    "            return s.f1\n",
    " \n",
    "       \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=20)\n",
    "       \n",
    "        p = study.best_params[\"p\"]\n",
    "        d = study.best_params[\"d\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        q_risk = study.best_params[\"q_risk\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "        model = ARIMAModel(p, d, q)\n",
    "        model.fit(train)\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend))\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q_risk, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"arima/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"arima/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95cdb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439bb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IForest Model \n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        \n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "                \n",
    "            model = IsolationForestModel()\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test))\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            \n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            s.process(preds, labels)\n",
    "        \n",
    "            if s.tp == 0 and s.fp == 0: return -1\n",
    "            if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "            if s.precision == 0 and s.recall == 0: return -1\n",
    "            if np.isnan(s.f1): return -1\n",
    "            return s.f1\n",
    " \n",
    "            \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=150)\n",
    "       \n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        model = IsolationForestModel()\n",
    "        model.fit(train)\n",
    "        scores = np.abs(model.get_scores(test))\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"iforest/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"iforest/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-scores.txt\", scores)\n",
    "        np.savetxt(save+\"-preds.txt\", preds)\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765ec1b",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08b776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regression Model \n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            lags = trial.suggest_int(\"lags\", 1, 5)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    " \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = RegressionModel(window, n_steps, lags)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            return compute_objective(labels, preds)\n",
    "\n",
    "       \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        lags = study.best_params[\"lags\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "        model = RegressionModel(window,n_steps, lags)\n",
    "        model.fit(train)\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend)[0])\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"regression/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"regression/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-scores.txt\", scores)\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef4025",
   "metadata": {},
   "source": [
    "### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8866c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# NBEATSModel\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "\n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = NBEATSModel(window, n_steps, use_gpu=True)            \n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x)[0], test)\n",
    "            return auc\n",
    " \n",
    "\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = jenkspy.jenks_breaks(scores, nb_class=20)[-2]\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"nbeats/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"nbeats/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be305c",
   "metadata": {},
   "source": [
    "### NHiTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d803257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x)[0], test)\n",
    "            return auc\n",
    " \n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = jenkspy.jenks_breaks(scores, nb_class=20)[-2]\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"nhits/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"nhits/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"nhits/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85e191",
   "metadata": {},
   "source": [
    "### RNN(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b6751-ac8d-4cb9-b62a-a22b744e502d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#supress output\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            s = ScoreCounter()\n",
    "            \n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "\n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = RNNModel(window, n_steps, rnn_model=\"GRU\")\n",
    "            \n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "            \n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x)[0], test)\n",
    "            return auc\n",
    " \n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = RNNModel(window, n_steps, use_gpu=True, rnn_model=\"GRU\")\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"rnn_gru/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"rnn_gru/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"rnn_gru/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b4567",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173ab58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 9, log=True)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "            \n",
    "            model = TCNModel(window, n_steps, use_gpu=True)\n",
    "            \n",
    "            model.fit(train)\n",
    "            auc, *_ = aumvc(lambda x: model.get_scores(x)[0], test)\n",
    "            return auc\n",
    " \n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = TCNModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = jenkspy.jenks_breaks(scores, nb_class=20)[-2]\n",
    " \n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"tcn/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"tcn/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"tcn/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30be85",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081b70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%capture\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 10, log=True)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], test))\n",
    "                \n",
    "            model = TransformerModel(window, n_steps, use_gpu=True)\n",
    "            model.fit(train)\n",
    "            scores = np.abs(model.get_scores(test_extend)[0])\n",
    "\n",
    "            # Get threshold (Not needed for Quantile)\n",
    "            thres = pot(scores, q, contam)\n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    " \n",
    "            return compute_objective(test, preds)\n",
    "        \n",
    "\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_params[\"window\"]\n",
    "        n_steps = study.best_params[\"n_steps\"]\n",
    "        q = study.best_params[\"q\"]\n",
    "        contam = study.best_params[\"contam\"]\n",
    "        \n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        model = TransformerModel(window, n_steps, use_gpu=True)\n",
    " \n",
    "        model.fit(train)\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(scores, q, contam)\n",
    "        \n",
    "        # Get predictions from threshold\n",
    "        preds = scores.copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+\"transformer/\"+f\n",
    "        os.makedirs(SAVE_DIR+\"transformer/\", exist_ok=True)\n",
    "        np.savetxt(save+\"-scores.txt\", scores, header=study.best_params.__str__())\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=study.best_params.__str__())\n",
    "\n",
    "        scorer.process(preds, labels)\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n",
    "    with open(SAVE_DIR+\"transformer/summary.txt\", 'a+') as summary:\n",
    "        summary.write(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c051f-6787-4d28-baea-d1849b63a651",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52c6679-4a62-4f35-b44e-65a9f8a6358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86, 12, 19016, 886, 0.08847736625514403, 0.0006306495690561278, 0.9993693504309439, 0.911522633744856, 0.8775510204081632, 0.08847736625514403, 0.16074766355140185\n",
      "61, 382, 18639, 918, 0.06230847803881512, 0.020083066084853583, 0.9799169339151464, 0.9376915219611849, 0.13769751693002258, 0.06230847803881512, 0.08579465541490858\n",
      "400, 291, 18709, 600, 0.4, 0.01531578947368421, 0.9846842105263158, 0.6, 0.5788712011577424, 0.4, 0.47309284447072736\n",
      "404, 688, 18408, 500, 0.4469026548672566, 0.03602848764139087, 0.9639715123586091, 0.5530973451327433, 0.36996336996337, 0.4469026548672566, 0.40480961923847697\n",
      "1636, 607, 17522, 235, 0.8743987172634955, 0.033482265982679685, 0.9665177340173203, 0.12560128273650453, 0.7293802942487739, 0.8743987172634955, 0.7953330092367525\n"
     ]
    }
   ],
   "source": [
    "model = \"transformer\"\n",
    "for path in PATHS:\n",
    "    scorer = ScoreCounter()\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    for f in file_list:\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "        preds = np.loadtxt(SAVE_DIR+f\"{model}/\"+f+\"-preds.txt\")\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d810f46-7a04-4280-866c-4b4e7b680309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
