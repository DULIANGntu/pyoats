{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8ef76-102b-40bc-a97c-041c0c63bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65a862-d3c9-4b75-982e-bbb911915a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view   \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b76147-753c-4ebf-a2f6-bb640dc34c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.generator.univariate import UnivariateDataGenerator\n",
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f18ac6-ebe4-4ff5-8083-f634936dbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3f1b1-0d8c-475d-8109-e6cf00745eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 40,10\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6190c0-fe43-4d6c-b0a0-7dabc2e12edd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load NASA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5aa60-1cdc-4a13-abc1-f5adf448acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"../data/nasa/train/\"\n",
    "TEST = \"../data/nasa/test/\"\n",
    "LABELS = \"../data/nasa/labeled_anomalies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b3ce0-c5de-45c5-8179-141d465cd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_class(x):\n",
    "    x = x.replace(\"contextual\", \"\\\"contextual\\\"\")\n",
    "    x = x.replace(\"point\", \"\\\"point\\\"\")\n",
    "    return x\n",
    "\n",
    "def make_array(row):\n",
    "    return np.array(row)\n",
    "\n",
    "converters = {\n",
    "    \"anomaly_sequences\": lambda x: make_array(ast.literal_eval(x)),\n",
    "    \"class\": lambda x: ast.literal_eval(convert_class(x))\n",
    "}\n",
    "\n",
    "\n",
    "label_df = pd.read_csv(LABELS, converters=converters)\n",
    "\n",
    "# get only those with multiple anomalies\n",
    "label_df = label_df[label_df[\"anomaly_sequences\"].apply(lambda x: x.shape[0]) > 1]\n",
    "label_df = label_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c74465-c8a6-4e57-8da7-440624e22a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c5d22-29d6-49eb-8fc2-d74bdc2e152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "for root, _, files in os.walk(TRAIN):\n",
    "    for file in files:\n",
    "        if file.split(\".\")[0] in label_df[\"chan_id\"].values:\n",
    "             train.append(os.path.join(root, file))\n",
    "                \n",
    "test=[]\n",
    "for root, _, files in os.walk(TEST):\n",
    "    for file in files:\n",
    "        if file.split(\".\")[0] in label_df[\"chan_id\"].values:\n",
    "             test.append(os.path.join(root, file))\n",
    "                \n",
    "                \n",
    "file_list = list(zip(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf2f52-1d55-4114-98d8-7cd0c7034cef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44271e33-0d56-4e7c-b812-c8ca44be5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = {}\n",
    "\n",
    "for idx, rng in enumerate(label_df[\"anomaly_sequences\"].values):\n",
    "    name = label_df.iloc[idx, 0]\n",
    "    test_arr = np.load(f\"{TEST}{name}.npy\")\n",
    "    \n",
    "    label = np.zeros(len(test_arr))\n",
    "    \n",
    "    for start, end in rng:\n",
    "        label[start:end] = 1\n",
    "    \n",
    "    data_labels.update({name: label})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ddcca-adce-4383-b2ab-1f18bde26237",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ceefc-97f6-4869-b916-299c5d32a818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for train, test in file_list:\n",
    "    train_arr = np.load(train)\n",
    "    test_arr = np.load(test)\n",
    "    data = np.vstack((train_arr, test_arr))\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(f\"{name} {'#'*30}\")\n",
    "    \n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = 40, 1 * data.shape[1]\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(data.shape[1])\n",
    "    axes[0].set_title(name)\n",
    "\n",
    "    for idx, d in enumerate(data.T):\n",
    "        axes[idx].plot(d)\n",
    "        \n",
    "    labels = label_df[label_df[\"chan_id\"] == name][\"anomaly_sequences\"].values[0]\n",
    "    for start, end in labels:\n",
    "        start += len(train_arr)\n",
    "        end += len(train_arr)\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.axvspan(start, end, alpha=0.5, color='red')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "\n",
    "\n",
    "    axes[-1].get_xaxis().set_visible(True)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824dc82-712b-43ed-93fd-318569eb2262",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412f15e-5dd6-404d-b780-1c33747fc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCounter:\n",
    "    def __init__(self, delay: int=None):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "        self.delay = delay\n",
    "        \n",
    "        self.point_anom_total = 0\n",
    "        self.point_anom_correct = 0\n",
    "        \n",
    "        self.contextual_anom_total = 0\n",
    "        self.contextual_anom_correct = 0\n",
    "        \n",
    "    def process(self, preds, labels, types):\n",
    "        preds = preds.copy()\n",
    "        labels = labels.copy()\n",
    "        \n",
    "        ground_truth_ones = np.where(labels == 1)[0]\n",
    "        pred_ones = np.where(preds == 1)[0]\n",
    "        ranges = self._consecutive(ground_truth_ones)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for idx, r in enumerate(ranges):\n",
    "            intersect = np.intersect1d(r, pred_ones, assume_unique=True)\n",
    "            # if alert delay more than 100 timesteps, count that as bad!\n",
    "            \n",
    "            if types[idx] == \"point\": self.point_anom_total += 1\n",
    "            else: self.contextual_anom_total += 1\n",
    "               \n",
    "            if intersect.size != 0:\n",
    "                cond = intersect[0] < r[0] + self.delay if self.delay is not None else True\n",
    "                if cond:\n",
    "                    tp += r.size\n",
    "                    if types[idx] == \"point\": self.point_anom_correct += 1\n",
    "                    else: self.contextual_anom_correct += 1\n",
    "                else:\n",
    "                    fn += r.size\n",
    " \n",
    "                preds[intersect] = 0\n",
    "                pred_ones = np.where(preds == 1)[0]\n",
    "            else:\n",
    "                fn += r.size\n",
    "            \n",
    "        fp += pred_ones.size\n",
    "        tn += preds.size - tp - fp - fn\n",
    "        \n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.tn += tn\n",
    "        self.fn += fn\n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _consecutive(self, data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self.tp/(self.fn+self.tp)\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self.fp/(self.tn+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def tnr(self):\n",
    "        return self.tn/(self.tn+self.fp)\n",
    "        \n",
    "    @property\n",
    "    def fnr(self):\n",
    "        return self.fn/(self.fn+self.tp)\n",
    "        \n",
    "    @property\n",
    "    def precision(self):\n",
    "        if self.tp+self.fp == 0: return 0\n",
    "        return self.tp/(self.tp+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def recall(self):\n",
    "        if self.tp+self.fn == 0: return 0\n",
    "        return self.tp/(self.tp+self.fn)\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        if self.recall + self.precision == 0: return 0\n",
    "        return (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "    \n",
    "    @property\n",
    "    def point_correct(self):\n",
    "        if self.point_anom_total == 0: return 0\n",
    "        return self.point_anom_correct/self.point_anom_total\n",
    "    \n",
    "    @property\n",
    "    def contextual_correct(self):\n",
    "        if self.contextual_anom_total == 0: return 0\n",
    "        return self.contextual_anom_correct/self.contextual_anom_total\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}, {scorer.point_correct}, {scorer.contextual_correct}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4010f89-221a-478b-8f6d-3c16bacfc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.zeros(20), np.full(20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0b834-d6e8-48b1-8df1-0ccc66d63f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.append(np.zeros(30), np.full(10, 1))\n",
    "pred[10] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f175664-7c17-4eb8-8eaf-3e2bcc9f6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 1\n",
    "tn = 19\n",
    "fn = 20\n",
    "\n",
    "scorer = ScoreCounter(delay=5)\n",
    "scorer.process(pred, labels, [\"point\"])\n",
    "\n",
    "assert tp == scorer.tp\n",
    "assert fp == scorer.fp\n",
    "assert tn == scorer.tn\n",
    "assert fn == scorer.fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1a345-50da-41d5-87ac-374d3faacc71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319d94a-eb41-456c-b845-f6296f0f5645",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1 Tuned (50% Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca001ffc-479b-4205-90f8-82ffe2025060",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b2f9c-f171-4ed9-9e46-afbea7aeee56",
   "metadata": {},
   "source": [
    "### Single-variate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3815b5b-7e5e-46ea-8789-347da1ffa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/multivar-nasa/f1tuned-1anom/\"\n",
    "DELAY = 50\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba924e-a8e9-4c48-85d5-4a0fe5535dc9",
   "metadata": {},
   "source": [
    "#### MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148482e-60e4-45a1-bcd4-6cd0d66c87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = MovingAverageModel(window)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for series in test_set.T:\n",
    "        score = np.abs(model.get_scores(series)[window:])\n",
    "        scores.append(score)\n",
    "\n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(score, q, contam)\n",
    "\n",
    "        pred = score.copy()\n",
    "        pred[pred <= thres] = 0\n",
    "        pred[pred > thres] = 1\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(preds, axis=0)\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983e9bb-ee2e-4acb-b55f-d119db493af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average Model\n",
    "model_name = \"ma\"\n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    \n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-1, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        test_extend = np.concatenate((train[-window:], tune_data))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "\n",
    "        s.process(preds, tune_labels, types)\n",
    "\n",
    "        if s.tp == 0 and s.fp == 0: return -1\n",
    "        if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "        if s.precision == 0 and s.recall == 0: return -1\n",
    "        if np.isnan(s.f1): return -1\n",
    "        return s.f1\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=150)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "\n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82fb966-d315-4d02-8852-a3027e178eb4",
   "metadata": {},
   "source": [
    "#### Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e86b67-0b9a-4105-b906-63886a76560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    thres = params[\"thres\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = QuantileModel(window, thres)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for series in test_set.T:\n",
    "        score = model.get_scores(series)[window:]\n",
    "        scores.append(score)\n",
    "        preds.append(score)\n",
    "        \n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(scores, axis=1)\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db851289-6fbb-4110-b297-f3aa076a7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"quantile\"\n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    \n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"thres\": trial.suggest_float(\"thres\", 0.9, 0.999),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-1, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        test_extend = np.concatenate((train[-params[\"window\"]:], tune_data))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "\n",
    "        s.process(preds, tune_labels, types)\n",
    "\n",
    "        if s.tp == 0 and s.fp == 0: return -1\n",
    "        if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "        if s.precision == 0 and s.recall == 0: return -1\n",
    "        if np.isnan(s.f1): return -1\n",
    "        return s.f1\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"thres\": study.best_params[\"thres\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "\n",
    "    test_extend = np.concatenate((train[-params[\"window\"]:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848897c1-800c-471b-9abd-8dd0a3058345",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b27704-b335-4bc1-a6f7-971ae1f55f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = RegressionModel(window)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for idx, series in enumerate(test_set.T):\n",
    "        model = RegressionModel(window)\n",
    "        model.fit(train_set.T[idx])\n",
    "\n",
    "        score = model.get_scores(series)[0].flatten()\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(score, q, contam)\n",
    "\n",
    "        pred = score.copy()\n",
    "        pred[pred <= thres] = 0\n",
    "        pred[pred > thres] = 1\n",
    "\n",
    "        preds.append(pred)\n",
    "        \n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(preds, axis=1)\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7001db-c4aa-496e-b2f0-3b2c9c748796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"regression\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-1, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        window = params[\"window\"]\n",
    "        test_extend = np.concatenate((train[-window:], tune_data))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "\n",
    "        s.process(preds, tune_labels, types)\n",
    "\n",
    "        if s.tp == 0 and s.fp == 0: return -1\n",
    "        if s.tp == 0 and s.fn == 0: return -1\n",
    "\n",
    "        if s.precision == 0 and s.recall == 0: return -1\n",
    "        if np.isnan(s.f1): return -1\n",
    "        return s.f1\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "\n",
    "    window = params[\"window\"]\n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c782ed5-c8d8-41b4-958e-e42f0ea927c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca5d79-e2fc-4c9e-a1f9-f57036df721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"regression\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"./results/multivar-nasa/f1tuned-1anom/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"preds\" not in i]\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    d = SAVE_DIR + method + f\n",
    "    with open(d) as file:\n",
    "        params = file.readline().strip().replace(\"# \", \"\").replace(\"'\", '\"')\n",
    "        params = json.loads(params)\n",
    "        \n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    path = file_list[idx][1]\n",
    "    data = np.load(path)\n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    scores = np.loadtxt(SAVE_DIR+method+f)\n",
    "   \n",
    "    preds = []\n",
    "    for score in scores:\n",
    "        thres = pot(score, params[\"q\"], params[\"contam\"])\n",
    "\n",
    "        pred = score.copy()\n",
    "        pred[pred <= thres] = 0\n",
    "        pred[pred > thres] = 1\n",
    "    \n",
    "        preds.append(pred)\n",
    "   \n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds = np.any(preds, axis=1)\n",
    "    \n",
    "    np.savetxt(d.replace(\"scores\", \"preds\"), preds, header=str(params))\n",
    "    \n",
    "#print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae231d5-5564-45f5-ab86-5b5459586a98",
   "metadata": {},
   "source": [
    "### Multivariate Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0fbbe-8b62-496c-ac9f-4781760d4a17",
   "metadata": {},
   "source": [
    "#### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e03a4-afa7-4062-bec2-6b262379353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = IsolationForestModel(window)\n",
    "    \n",
    "    test_extend = np.concatenate((train_set[-window:], test_set))\n",
    "    model = IsolationForestModel(window)\n",
    "    model.fit(train_set)\n",
    "    score = model.get_scores(test_extend).flatten()[window:]\n",
    "    \n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e22713-2f92-4d31-bd95-7c9cc4bcd204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"iforest\"\n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    \n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    i = 0\n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        \n",
    "        s.process(preds, tune_labels, types)\n",
    "        \n",
    "        return s.f1\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=150)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b6597-8a14-49ac-990f-0970b56fe4b1",
   "metadata": {},
   "source": [
    "#### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899895-6da3-49ad-aa8d-1084f859bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad913a2e-71e0-4423-8a61-5b9c8078e837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"nbeats\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        s.process(preds, tune_labels, types)\n",
    "        return s.f1\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", res, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc2036-af36-4231-b9fa-9caab4956713",
   "metadata": {},
   "source": [
    "#### NHiTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf0f65-3e16-423f-b29c-62ded3d01aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4593886-d489-47dc-8b77-937e32546bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"nhits\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        s.process(preds, tune_labels, types)\n",
    "        return s.f1\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", res, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6939a-269b-4071-9bed-b62eb2518542",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RNN-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a404f8d-6b72-4b05-a1a0-d05d4927f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = RNNModel(window, n_steps, use_gpu=True, rnn_model=\"GRU\")\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c20b6-1803-42b8-b419-5b999d529458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"rnn_gru\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        s.process(preds, tune_labels, types)\n",
    "        return s.f1\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0b5f6-993f-4ccd-a969-432cfcfca2d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcf1d2-35bd-4a85-a418-d69f17c6ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = TCNModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef9031-842d-4b6a-a0b4-c799cea9a4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"tcn\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        s.process(preds, tune_labels, types)\n",
    "        return s.f1\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895d617-5946-4424-9c73-afbb75667855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec96de02-4062-429b-87a1-c554979355c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c036b7-3a71-4ccb-b7e8-59f19449c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = TransformerModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fc154-cff0-42e6-9089-a9acd5606622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"transformer\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, tune_data, params)\n",
    "        s.process(preds, tune_labels, types)\n",
    "        return s.f1\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781b8c3-ad2a-41f0-9b06-dae4e44614d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"transformer\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"./results/multivar-nasa/f1tuned-1anom/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"preds\" not in i]\n",
    "\n",
    "params_file = {}\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    with open(SAVE_DIR+method+f, 'r') as file:\n",
    "        params = file.readline()\n",
    "        \n",
    "    params = params.strip()\n",
    "    params = params.replace(\"# \", \"\")\n",
    "    params = params.replace(\"'\", '\"')\n",
    "    params = json.loads(params)\n",
    "    \n",
    "    params_file.update({name: params})\n",
    "\n",
    "print(params)\n",
    "    \n",
    "#######################\n",
    "print(\"here\")\n",
    "#######################\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    test_path = file_list[idx][1]\n",
    "    test = np.load(test_path)\n",
    "    \n",
    "    train_path = file_list[idx][0]\n",
    "    train = np.load(train_path)\n",
    "    \n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    params = params_file[name]\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "    save = f\"{SAVE_DIR}{method}/{name}\"\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab661b1b-80f7-4901-9e16-f20c3abde9ef",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc699d3-55cf-49dd-a4e5-bb91d5bc2073",
   "metadata": {},
   "source": [
    "## Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5f3d27ce-179c-4d0d-bcf9-2cfa684092ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2436, 914, 96954, 4465, 0.3529923199536299, 0.00933910982139208, 0.9906608901786079, 0.6470076800463701, 0.7271641791044776, 0.3529923199536299, 0.4752707052970442, 0.26666666666666666, 0.4230769230769231\n"
     ]
    }
   ],
   "source": [
    "method = \"tranad\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"../results/multivar-nasa/untuned/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"scores\" not in i]\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    path = file_list[idx][1]\n",
    "    data = np.load(path)\n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    preds = np.loadtxt(SAVE_DIR+method+f)\n",
    "    \n",
    "    scorer.process(preds, label, types)\n",
    "    \n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4e116-c435-419c-969c-e8f44dafebdb",
   "metadata": {},
   "source": [
    "## F1 - 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed534e-1bde-40d9-9b30-43e96b941839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = \"regression\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"../results/multivar-nasa/f1tuned-1anom/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"scores\" not in i]\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    path = file_list[idx][1]\n",
    "    data = np.load(path)\n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    preds = np.loadtxt(SAVE_DIR+method+f)\n",
    "    \n",
    "    scorer.process(preds, label, types)\n",
    "    \n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367416b-3ba5-4b9e-a5f7-889ede498230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
