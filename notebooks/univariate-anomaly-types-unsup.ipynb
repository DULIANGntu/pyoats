{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628e8b40-08c5-49a7-9048-1108cb23b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd21ec7-7f01-451c-8875-c0ed6411218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view   \n",
    "import optuna\n",
    "import scipy.signal as signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ac4e33-81a1-4792-9705-504ad4867cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.generator.univariate import UnivariateDataGenerator\n",
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *\n",
    "from one.scorer.spot import *\n",
    "import jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8ad3c7-97d8-47a1-bcae-e3aac46138d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c521f5-f82a-420f-af7a-979c163caf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 40,10\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24463bb6-4cc9-48f4-9b92-5781b967cc1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a412f15e-5dd6-404d-b780-1c33747fc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCounter:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "    def process(self, preds, labels):\n",
    "        preds = preds.copy()\n",
    "        labels = labels.copy()\n",
    "        ground_truth_ones = np.where(labels == 1)[0]\n",
    "        pred_ones = np.where(preds == 1)[0]\n",
    "        \n",
    "        ranges = self._consecutive(ground_truth_ones)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for r in ranges:\n",
    "            intersect = np.intersect1d(r, pred_ones, assume_unique=True)\n",
    "            if intersect.size != 0:\n",
    "                tp += r.size\n",
    "                preds[intersect] = 0\n",
    "                pred_ones = np.where(preds == 1)[0]\n",
    "            else:\n",
    "                fn += r.size\n",
    "            \n",
    "        fp += pred_ones.size\n",
    "        tn += preds.size - tp - fp - fn\n",
    "        \n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.tn += tn\n",
    "        self.fn += fn\n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _consecutive(self, data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self.tp/(self.fn+self.tp)\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self.fp/(self.tn+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def tnr(self):\n",
    "        return self.tn/(self.tn+self.fp)\n",
    "        \n",
    "    @property\n",
    "    def fnr(self):\n",
    "        return self.fn/(self.fn+self.tp)\n",
    "        \n",
    "    @property\n",
    "    def precision(self):\n",
    "        return self.tp/(self.tp+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def recall(self):\n",
    "        return self.tp/(self.tp+self.fn)\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        if self.precision + self.recall == 0: return 0\n",
    "        return (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf463b27-acc7-4e6f-aa00-18e3d88ad70a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c420a-9b89-427c-ae90-3021427a3313",
   "metadata": {},
   "source": [
    "### AUMVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0519a10f-1b50-4d3c-8730-1063eb36b900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNSUPERVISED LEARNING METRICS\n",
    "Author:     Bob Stienen\n",
    "License:    MIT License\n",
    "Source:     http://www.github.com/bstienen/AUMVC\n",
    "\n",
    "Implementation of the Area under the Mass-Volume Curve algorithm as by\n",
    "- Stephan Clémençon and Jeremie Jakubowicz, Scoring anomalies: a M-estimation\n",
    "  formulation approach. 2013-04\n",
    "\n",
    "Implementation is inspired by\n",
    "   https://github.com/albertcthomas/anomaly_tuning\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def aumvc(scoring_function,\n",
    "          X_test,\n",
    "          N_mc=100000,\n",
    "          N_levelsets=100,\n",
    "          normalise=True):\n",
    "    \"\"\" Calculate the area under the mass-volume curve for an anomaly detection\n",
    "    function or algorithm\n",
    "\n",
    "    This function uses monte carlo sampling in the parameter space box spanned\n",
    "    by the provided test data in order to estimate the level set of the\n",
    "    scoring function. For higher dimensionalities the amount of sampled data\n",
    "    points would yield this algorithm intractable. In these cases the use of\n",
    "    the `aumvc_hd` function is advised instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scoring_function: function\n",
    "        Function that takes datapoints as numpy.ndarray (nPoints, nFeatures)\n",
    "        and returns an anomaly score. This score should be in range [0,1],\n",
    "        where 1 indicates the point not being an anomaly (and 0 that the point\n",
    "        *is* an anomaly).\n",
    "    X_test: numpy.ndarray of shape (nPoints, nFeatures)\n",
    "        Datapoints used for testing the algorithm.\n",
    "    N_mc: int (default: 100,000)\n",
    "        Number of datapoints to sample in the parameter space to estimate the\n",
    "        level sets of the scoring function.\n",
    "    N_levelsets: int (default: 100)\n",
    "        Number of level sets to evaluate.\n",
    "    normalise: bool (default: True)\n",
    "        Indicates if output scores of the scoring_function should be normalised\n",
    "        before calculating the mass-volume curve. \"\"\"\n",
    "\n",
    "    # Get ranges for the test data\n",
    "    mins = np.amin(X_test, axis=0)\n",
    "    maxs = np.amax(X_test, axis=0)\n",
    "    \n",
    "    if X_test.ndim==1:\n",
    "        mins = np.array([mins])\n",
    "        maxs = np.array([maxs])\n",
    "       \n",
    "\n",
    "    # Generate uniform MC data\n",
    "    U = np.random.rand(N_mc, len(mins))*(maxs-mins)+mins\n",
    "\n",
    "    # Calculate volume of total cube\n",
    "    vol_tot_cube = np.prod(maxs - mins)\n",
    "\n",
    "    # Score test and MC data\n",
    "    score_U = scoring_function(U)\n",
    "    score_test = scoring_function(X_test)\n",
    "\n",
    "    # Do normalising if needed\n",
    "    if normalise:\n",
    "        minimum = min(np.amin(score_U), np.amin(score_test))\n",
    "        maximum = max(np.amax(score_U), np.amax(score_test))\n",
    "        score_U = (score_U - minimum) / (maximum - minimum)\n",
    "        score_test = (score_test - minimum) / (maximum - minimum)\n",
    "\n",
    "    # Calculate alphas to use\n",
    "    alphas = np.linspace(0, 1, N_levelsets)\n",
    "\n",
    "    # Compute offsets\n",
    "    offsets = np.percentile(score_test, 100 * (1 - alphas))\n",
    "\n",
    "    # Compute volumes of associated level sets\n",
    "    volumes = (np.array([np.mean(score_U >= offset)\n",
    "                        for offset in offsets]) * vol_tot_cube)\n",
    "\n",
    "    # Calculating area under the curve\n",
    "    area = auc(alphas, volumes)\n",
    "\n",
    "    # Return area and curve variables\n",
    "    return (area, alphas, volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "da9b9413-b9e6-49ee-b40d-35b578f0843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a559a-4091-4a4e-acdf-2ef03d8732fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dffae82d-f862-4dc5-ba68-7404382d777b",
   "metadata": {},
   "source": [
    "### Label-Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "516385ce-c259-4062-ae1c-a954062a25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualitative_metrics(data, preds, window=10):\n",
    "    \"\"\"\n",
    "    Returns a few quantitative metrics for us to use for evaluation when labels\n",
    "    are not provided.\n",
    "    Parameters\n",
    "    -----------\n",
    "    df: pd.Dataframe\n",
    "        the dataframe with 'timestamp', 'value' and 'predict' columns\n",
    "        where 'predict' is 1 for those predicted as anomalous and 0 otherwise.\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple\n",
    "        (number of anomalies,\n",
    "        % of anomnalies,\n",
    "        avg. distance between mean and all anomalies (yaxis),\n",
    "        avg. time distance between consecutive anomalies,\n",
    "        avg. cycle distance between consecutive anomalies,\n",
    "        maximum range between non anomaly points (yaxis)\n",
    "        )\n",
    "    \"\"\"\n",
    "    num_anomalies = preds.sum()\n",
    "    percent_anomalies = num_anomalies/len(preds)\n",
    "\n",
    "    mean_val = data.mean(axis=0)\n",
    "\n",
    "    pred_anomalies = data[preds == 1]\n",
    "    pred_non_anomalies = data[preds == 0]\n",
    "\n",
    "    avg_anom_dist_from_mean_val = np.linalg.norm(pred_anomalies - mean_val, axis=-1).mean()\n",
    "    \n",
    "    \n",
    "    if 1 < (preds==1).sum() < len(data):\n",
    "        avg_cycles_delta_between_anomalies = np.diff(np.where(preds==1)[0]).mean()\n",
    "    else: \n",
    "        avg_cycles_delta_between_anomalies = 0\n",
    "        \n",
    "    if 0 < (preds==1).sum() < len(data):\n",
    "        max_range_non_anomalies = (np.abs(pred_non_anomalies.max() - pred_non_anomalies.min())).mean()\n",
    "    else: \n",
    "        max_range_non_anomalies = 1e5\n",
    " \n",
    "     \n",
    "    \n",
    "    # make sure window is odd\n",
    "    if window % 2 == 0: window += 1\n",
    "    padding = window//2\n",
    "    \n",
    "    # local difference filter\n",
    "    fil = window\n",
    "    fil = np.full((window),-1/(window-1))\n",
    "    fil[padding] = 1\n",
    "    if data.ndim > 1:\n",
    "        fil = np.tile(fil, (data.shape[1], 1)).T\n",
    " \n",
    "        \n",
    "    # maximize\n",
    "    # abs-trend avg\n",
    "    grads = signal.savgol_filter(data, window, 1, deriv=1, axis=0)\n",
    "    grads = np.abs(grads)\n",
    "    conv = np.abs(signal.convolve(grads, fil, mode=\"valid\"))\n",
    "    conv = np.pad(conv, (padding, padding), mode=\"edge\")\n",
    "    diff_mean_trend = (conv[preds==1].mean(axis=0) - conv[preds==0].mean(axis=0)).sum() if 0<(preds == 1).sum()<preds.size else 0\n",
    "    \n",
    "    # maximize\n",
    "    # ~= midpoint minus avg. of sides\n",
    "    conv = np.abs(signal.convolve(data, fil, mode=\"valid\"))\n",
    "    conv = np.pad(conv, (padding, padding), mode=\"edge\")\n",
    "    diff_mid_avg = (conv[preds==1].mean(axis=0) - conv[preds==0].mean(axis=0)).sum() if 0<(preds == 1).sum()<preds.size else 0\n",
    "\n",
    "    return (\n",
    "            percent_anomalies, # min\n",
    "            avg_anom_dist_from_mean_val, #max\n",
    "            avg_cycles_delta_between_anomalies, #max\n",
    "            max_range_non_anomalies, #max\n",
    "            diff_mean_trend, #max\n",
    "            diff_mid_avg #max\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "8aeb3e5e-d890-48e1-875d-34ef5a2ad105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tune_data\n",
    "preds = preds\n",
    "\n",
    "window=10\n",
    "if window % 2 == 0: window += 1\n",
    "padding = window//2\n",
    " \n",
    "fil = window\n",
    "fil = np.full((window),-1/(window-1))\n",
    "fil[padding] = 1\n",
    "if data.ndim > 1:\n",
    "    fil = np.tile(fil, (data.shape[1], 1)).T\n",
    "\n",
    "grads = signal.savgol_filter(data, window, 1, deriv=1, axis=0)\n",
    "grads = np.abs(grads)\n",
    "conv = np.abs(signal.convolve(grads, fil, mode=\"valid\"))\n",
    "conv = np.pad(conv, (padding, padding), mode=\"edge\")\n",
    "diff_mean_trend = (conv[preds==1].mean(axis=0) - conv[preds==0].mean(axis=0)).sum() if 0<(preds == 1).sum()<preds.size else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e7322-4b76-4857-9919-230823581a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8acc2-00db-4d94-bf16-a63d22043a7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F1 Tuned - 50% dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a3c1e-7a7f-4cf2-a2bb-e596baa6f8d3",
   "metadata": {},
   "source": [
    "### -- Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef0fb3f4-9acd-4b99-94a9-9068c8ef2601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *\n",
    "from numpy.lib.stride_tricks import sliding_window_view   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424b29d3-c4a0-4d09-83f0-904dce2ed7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH0 = \"../data/univar-synth/point_global/\"\n",
    "PATH1 = \"../data/univar-synth/point_contextual/\"\n",
    "PATH2 = \"../data/univar-synth/collective_global/\"\n",
    "PATH3 = \"../data/univar-synth/collective_trend/\"\n",
    "PATH4 = \"../data/univar-synth/collective_seasonal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7509bfcb-ac94-4ff9-a492-ae5609d53aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [PATH0, PATH1, PATH2, PATH3, PATH4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986822e6-c5e0-482f-b200-27f4fa17ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/univar-synth/unsupervised-metrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51bd02e3-2b7d-41da-8ca6-f8ca5dacd656",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35090c0b-683d-41f0-bcac-0955143aabe2",
   "metadata": {},
   "source": [
    "### MA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d1c28-0535-4770-b0e7-5e21d6b67b9b",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "1c514753-fcd2-4983-bac6-69daabf7610f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285, 389, 9121, 205, 0.5816326530612245, 0.040904311251314406, 0.9590956887486856, 0.41836734693877553, 0.4228486646884273, 0.5816326530612245, 0.4896907216494846\n",
      "48, 1295, 8231, 426, 0.10126582278481013, 0.13594373294142348, 0.8640562670585765, 0.8987341772151899, 0.035740878629932984, 0.10126582278481013, 0.05283434232250963\n",
      "200, 414, 9086, 300, 0.4, 0.04357894736842105, 0.956421052631579, 0.6, 0.3257328990228013, 0.4, 0.3590664272890485\n",
      "100, 2487, 7313, 100, 0.5, 0.2537755102040816, 0.7462244897959184, 0.5, 0.038654812524159254, 0.5, 0.07176175098672408\n",
      "767, 489, 8744, 0, 1.0, 0.05296220080147298, 0.947037799198527, 0.0, 0.6106687898089171, 1.0, 0.7582797825012357\n"
     ]
    }
   ],
   "source": [
    "# MA Model \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "Qualitative Metrics:\n",
    "---------------------------------------\n",
    "    avg_anom_dist_from_mean_val\n",
    "    avg_cycles_delta_between_anomalies\n",
    "    max_range_non_anomalies\n",
    "    diff_mean_trend\n",
    "    midpoint_avg_difference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "idx_dict = {\n",
    "    \"avg_anom_dist_from_mean_val\": 1,\n",
    "    \"avg_cycles_delta_between_anomalies\": 2,\n",
    "    \"max_range_non_anomalies\": 3,\n",
    "    \"max_range_non_anomalies-min\": 3,\n",
    "    \"diff_mean_trend\": 4,\n",
    "    \"diff_mid_avg\": 5,\n",
    "\n",
    "}\n",
    "\n",
    "metric_name = \"aumvc-max_range_non_anomalies-min\"\n",
    "\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        n_tune = labels.size // 2\n",
    "        tune_data, tune_labels = test[:n_tune], labels[:n_tune]\n",
    "        test_data, test_labels = test[n_tune:], labels[n_tune:]\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], tune_data))\n",
    "            model = MovingAverageModel(window)\n",
    "            scores = np.abs(model.get_scores(test_extend)[window:])\n",
    "            \n",
    "            aumvc_metric = aumvc(lambda x: np.abs(model.get_scores(x.flatten()))[window:], tune_data)[0] if \"aumvc\" in metric_name else 0\n",
    "            \n",
    "            #get threshold\n",
    "            thres = pot(scores, q, contam) # pot\n",
    "            \n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            metric = qualitative_metrics(tune_data, preds, window)\n",
    "            return metric[idx_dict[metric_name.replace(\"aumvc-\", \"\")]], aumvc_metric\n",
    "        \n",
    "         \n",
    "        study = optuna.create_study(directions=[\"minimize\", \"minimize\"])\n",
    "        study.optimize(objective, n_trials=200)\n",
    "       \n",
    "        window = study.best_trials[0].params[\"window\"]\n",
    "        contam = study.best_trials[0].params[\"contam\"]\n",
    "        q = study.best_trials[0].params[\"q\"]\n",
    "        \n",
    "        model = MovingAverageModel(window)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend)[window:] )\n",
    "        \n",
    "        #get threshold\n",
    "        thres = pot(scores[:n_tune], q, contam) # pot\n",
    "\n",
    "        preds = scores[n_tune:].copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels[n_tune:])\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+f\"ma/{metric_name}/\"+f\n",
    "        os.makedirs(SAVE_DIR+f\"ma/{metric_name}\", exist_ok=True)\n",
    "        params = study.best_trials[0].params\n",
    "        params.update({\"thres\": thres})\n",
    "        np.savetxt(save+\"-scores.txt\", scores[n_tune:], header=str(params))\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063668c8-3402-43ee-83b7-db68cb1ec362",
   "metadata": {},
   "source": [
    "#### Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f05def-0f99-4518-b728-b4a61292c078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MA Model \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "Qualitative Metrics:\n",
    "---------------------------------------\n",
    "    avg_anom_dist_from_mean_val\n",
    "    avg_cycles_delta_between_anomalies\n",
    "    max_range_non_anomalies\n",
    "    diff_mean_trend\n",
    "    midpoint_avg_difference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "idx_dict = {\n",
    "    \"avg_anom_dist_from_mean_val\": 1,\n",
    "    \"avg_cycles_delta_between_anomalies\": 2,\n",
    "    \"max_range_non_anomalies-min\": 3,\n",
    "    \"diff_mean_trend\": 4,\n",
    "    \"diff_mid_avg\": 5,\n",
    "\n",
    "}\n",
    "\n",
    "metric_name = \"combo-01234-min_max_max_min_max\"\n",
    "\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        n_tune = labels.size // 2\n",
    "        tune_data, tune_labels = test[:n_tune], labels[:n_tune]\n",
    "        test_data, test_labels = test[n_tune:], labels[n_tune:]\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], tune_data))\n",
    "            model = MovingAverageModel(window)\n",
    "            scores = np.abs(model.get_scores(test_extend)[window:])\n",
    "            \n",
    "            #aumvc_metric = aumvc(lambda x: np.abs(model.get_scores(x.flatten()))[window:], tune_data)[0] if \"aumvc\" in metric_name else 0\n",
    "            \n",
    "            #get threshold\n",
    "            thres = pot(scores, q, contam) # pot\n",
    "            \n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            metric = qualitative_metrics(tune_data, preds, window)\n",
    "            return metric[0], metric[1], metric[2], metric[3], metric[4]\n",
    "        \n",
    "         \n",
    "        study = optuna.create_study(directions=[\"minimize\", \"maximize\", \"maximize\", \"minimize\",  \"maximize\"])\n",
    "        study.optimize(objective, n_trials=200)\n",
    "       \n",
    "        window = study.best_trials[0].params[\"window\"]\n",
    "        contam = study.best_trials[0].params[\"contam\"]\n",
    "        q = study.best_trials[0].params[\"q\"]\n",
    "        \n",
    "        model = MovingAverageModel(window)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = np.abs(model.get_scores(test_extend)[window:] )\n",
    "        \n",
    "        #get threshold\n",
    "        thres = pot(scores[:n_tune], q, contam) # pot\n",
    "\n",
    "        preds = scores[n_tune:].copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels[n_tune:])\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+f\"ma/{metric_name}/\"+f\n",
    "        os.makedirs(SAVE_DIR+f\"ma/{metric_name}\", exist_ok=True)\n",
    "        params = study.best_trials[0].params\n",
    "        params.update({\"thres\": thres})\n",
    "        np.savetxt(save+\"-scores.txt\", scores[n_tune:], header=str(params))\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3b772-fe57-45f6-b41d-115779504624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 20:44:40 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[2022-08-10 20:44:42,501] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 758 samples.\n",
      "[2022-08-10 20:44:42,501] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 758 samples.\n",
      "2022-08-10 20:44:42 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 758 samples.\n",
      "[2022-08-10 20:44:42,591] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "[2022-08-10 20:44:42,591] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:44:42 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:44:42 pytorch_lightning.utilities.rank_zero INFO: GPU available: True, used: True\n",
      "2022-08-10 20:44:42 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-08-10 20:44:42 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-08-10 20:44:42 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-08-10 20:44:42 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-08-10 20:44:42 pytorch_lightning.callbacks.model_summary INFO: \n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | criterion | MSELoss    | 0     \n",
      "1 | stacks    | ModuleList | 6.3 M \n",
      "-----------------------------------------\n",
      "6.3 M     Trainable params\n",
      "1.5 K     Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.345    Total estimated model params size (MB)\n",
      "2022-08-10 20:45:12 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[2022-08-10 20:45:17,736] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 695 samples.\n",
      "[2022-08-10 20:45:17,736] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 695 samples.\n",
      "2022-08-10 20:45:17 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 695 samples.\n",
      "[2022-08-10 20:45:17,829] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "[2022-08-10 20:45:17,829] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:45:17 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:45:17 pytorch_lightning.utilities.rank_zero INFO: GPU available: True, used: True\n",
      "2022-08-10 20:45:17 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-08-10 20:45:17 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-08-10 20:45:17 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-08-10 20:45:17 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-08-10 20:45:17 pytorch_lightning.callbacks.model_summary INFO: \n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | criterion | MSELoss    | 0     \n",
      "1 | stacks    | ModuleList | 6.8 M \n",
      "-----------------------------------------\n",
      "6.8 M     Trainable params\n",
      "1.9 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.265    Total estimated model params size (MB)\n",
      "2022-08-10 20:45:45 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[2022-08-10 20:45:47,021] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 677 samples.\n",
      "[2022-08-10 20:45:47,021] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 677 samples.\n",
      "2022-08-10 20:45:47 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 677 samples.\n",
      "[2022-08-10 20:45:47,117] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "[2022-08-10 20:45:47,117] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:45:47 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:45:47 pytorch_lightning.utilities.rank_zero INFO: GPU available: True, used: True\n",
      "2022-08-10 20:45:47 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-08-10 20:45:47 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-08-10 20:45:47 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-08-10 20:45:47 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-08-10 20:45:47 pytorch_lightning.callbacks.model_summary INFO: \n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | criterion | MSELoss    | 0     \n",
      "1 | stacks    | ModuleList | 7.0 M \n",
      "-----------------------------------------\n",
      "7.0 M     Trainable params\n",
      "2.0 K     Non-trainable params\n",
      "7.0 M     Total params\n",
      "27.892    Total estimated model params size (MB)\n",
      "2022-08-10 20:48:55 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[2022-08-10 20:48:56,873] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 697 samples.\n",
      "[2022-08-10 20:48:56,873] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 697 samples.\n",
      "2022-08-10 20:48:56 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 697 samples.\n",
      "[2022-08-10 20:48:56,967] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "[2022-08-10 20:48:56,967] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:48:56 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 32-bits; casting model to float32.\n",
      "2022-08-10 20:48:56 pytorch_lightning.utilities.rank_zero INFO: GPU available: True, used: True\n",
      "2022-08-10 20:48:56 pytorch_lightning.utilities.rank_zero INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-08-10 20:48:56 pytorch_lightning.utilities.rank_zero INFO: IPU available: False, using: 0 IPUs\n",
      "2022-08-10 20:48:56 pytorch_lightning.utilities.rank_zero INFO: HPU available: False, using: 0 HPUs\n",
      "2022-08-10 20:48:57 pytorch_lightning.accelerators.gpu INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2022-08-10 20:48:57 pytorch_lightning.callbacks.model_summary INFO: \n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | criterion | MSELoss    | 0     \n",
      "1 | stacks    | ModuleList | 6.8 M \n",
      "-----------------------------------------\n",
      "6.8 M     Trainable params\n",
      "1.9 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.263    Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# NBEATS Model \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"\n",
    "Qualitative Metrics:\n",
    "---------------------------------------\n",
    "    avg_anom_dist_from_mean_val\n",
    "    avg_cycles_delta_between_anomalies\n",
    "    max_range_non_anomalies\n",
    "    diff_mean_trend\n",
    "    midpoint_avg_difference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "idx_dict = {\n",
    "    \"avg_anom_dist_from_mean_val\": 1,\n",
    "    \"avg_cycles_delta_between_anomalies\": 2,\n",
    "    \"max_range_non_anomalies-min\": 3,\n",
    "    \"diff_mean_trend\": 4,\n",
    "    \"diff_mid_avg\": 5,\n",
    "\n",
    "}\n",
    "\n",
    "metric_name = \"combo-1234-max_max_min_max\"\n",
    "\n",
    "\n",
    "for path in PATHS:\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    scorer = ScoreCounter()\n",
    "    for f in file_list:\n",
    "        train = np.loadtxt(path+f+\"-train.txt\")\n",
    "        test = np.loadtxt(path+f+\"-test.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")\n",
    "\n",
    "        n_tune = labels.size // 2\n",
    "        tune_data, tune_labels = test[:n_tune], labels[:n_tune]\n",
    "        test_data, test_labels = test[n_tune:], labels[n_tune:]\n",
    "        \n",
    "        def objective(trial):\n",
    "            window = trial.suggest_int(\"window\", 10, 150)\n",
    "            n_steps = trial.suggest_int(\"n_steps\", 1, 5, log=True)\n",
    "            contam = trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "            q = trial.suggest_float(\"q\", 1e-5, 1e-1, log=True)\n",
    "            \n",
    "            test_extend = np.concatenate((train[-window:], tune_data))\n",
    "            model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    "            model.fit(train)\n",
    "            scores = model.get_scores(test_extend)[0]\n",
    "            \n",
    "            \n",
    "            #get threshold\n",
    "            thres = pot(scores, q, contam) # pot\n",
    "            \n",
    "            preds = scores.copy()\n",
    "            preds[preds <= thres] = 0\n",
    "            preds[preds > thres] = 1\n",
    "            \n",
    "            metric = qualitative_metrics(tune_data, preds, window)\n",
    "            return  metric[1], metric[2], metric[3], metric[4]\n",
    "        \n",
    "         \n",
    "        study = optuna.create_study(directions=[\"maximize\", \"maximize\", \"minimize\", \"maximize\"])\n",
    "        study.optimize(objective, n_trials=35)\n",
    "       \n",
    "        window = study.best_trials[0].params[\"window\"]\n",
    "        n_steps = study.best_trials[0].params[\"n_steps\"]\n",
    "        contam = study.best_trials[0].params[\"contam\"]\n",
    "        q = study.best_trials[0].params[\"q\"]\n",
    "        \n",
    "        model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    "        model.fit(train)\n",
    "        \n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        scores = model.get_scores(test_extend)[0]\n",
    "        \n",
    "        #get threshold\n",
    "        thres = pot(scores[:n_tune], q, contam) # pot\n",
    "\n",
    "        preds = scores[n_tune:].copy()\n",
    "        preds[preds <= thres] = 0\n",
    "        preds[preds > thres] = 1\n",
    "\n",
    "        scorer.process(preds, labels[n_tune:])\n",
    "        \n",
    "        # Save results\n",
    "        save = SAVE_DIR+f\"nbeats/{metric_name}/\"+f\n",
    "        os.makedirs(SAVE_DIR+f\"nbeats/{metric_name}\", exist_ok=True)\n",
    "        params = study.best_trials[0].params\n",
    "        params.update({\"thres\": thres})\n",
    "        np.savetxt(save+\"-scores.txt\", scores[n_tune:], header=str(params))\n",
    "        np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "\n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c051f-6787-4d28-baea-d1849b63a651",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c6679-4a62-4f35-b44e-65a9f8a6358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/univar-synth/unsupervised-metrics/\"\n",
    "model = \"nbeats\"\n",
    "metric = \"combo-1234-max_max_min_max\"\n",
    "for path in PATHS:\n",
    "    scorer = ScoreCounter()\n",
    "    file_list = [\"-\".join(f.split(\"-\")[:-1]) for f in get_files_from_path(path) if \"train\" in f]\n",
    "    for f in file_list:\n",
    "        preds = np.loadtxt(SAVE_DIR+f\"{model}/{metric}/\"+f+\"-preds.txt\")\n",
    "        labels = np.loadtxt(path+f+\"-labels.txt\")[-len(preds):]\n",
    "        scorer.process(preds, labels)\n",
    "        \n",
    "    print(f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a47f9-c1df-472f-96ac-d8038c4ec72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e06b3-3f66-47a5-a0c0-421287d43ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea9b6c-2f37-48e9-91cf-c1ec558bd0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e506d-02c4-4f55-a72e-0391095e3e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343e9da-8f12-4658-a0f9-ace79a9daeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e0630-a5f4-4c59-b289-9d454564a56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec35a48-202c-457d-bd55-32da5d6a15a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
