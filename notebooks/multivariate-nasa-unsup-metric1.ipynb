{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2544f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view   \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.generator.univariate import UnivariateDataGenerator\n",
    "from one.models import *\n",
    "from one.utils import *\n",
    "from one.scorer.pot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 40,10\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57a0f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load NASA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"../data/nasa/train/\"\n",
    "TEST = \"../data/nasa/test/\"\n",
    "LABELS = \"../data/nasa/labeled_anomalies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_class(x):\n",
    "    x = x.replace(\"contextual\", \"\\\"contextual\\\"\")\n",
    "    x = x.replace(\"point\", \"\\\"point\\\"\")\n",
    "    return x\n",
    "\n",
    "def make_array(row):\n",
    "    return np.array(row)\n",
    "\n",
    "converters = {\n",
    "    \"anomaly_sequences\": lambda x: make_array(ast.literal_eval(x)),\n",
    "    \"class\": lambda x: ast.literal_eval(convert_class(x))\n",
    "}\n",
    "\n",
    "\n",
    "label_df = pd.read_csv(LABELS, converters=converters)\n",
    "\n",
    "# get only those with multiple anomalies\n",
    "label_df = label_df[label_df[\"anomaly_sequences\"].apply(lambda x: x.shape[0]) > 1]\n",
    "label_df = label_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f053af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "for root, _, files in os.walk(TRAIN):\n",
    "    for file in files:\n",
    "        if file.split(\".\")[0] in label_df[\"chan_id\"].values:\n",
    "             train.append(os.path.join(root, file))\n",
    "                \n",
    "test=[]\n",
    "for root, _, files in os.walk(TEST):\n",
    "    for file in files:\n",
    "        if file.split(\".\")[0] in label_df[\"chan_id\"].values:\n",
    "             test.append(os.path.join(root, file))\n",
    "                \n",
    "                \n",
    "file_list = list(zip(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7644da7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033dcee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = {}\n",
    "\n",
    "for idx, rng in enumerate(label_df[\"anomaly_sequences\"].values):\n",
    "    name = label_df.iloc[idx, 0]\n",
    "    test_arr = np.load(f\"{TEST}{name}.npy\")\n",
    "    \n",
    "    label = np.zeros(len(test_arr))\n",
    "    \n",
    "    for start, end in rng:\n",
    "        label[start:end] = 1\n",
    "    \n",
    "    data_labels.update({name: label})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4a79d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fef8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for train, test in file_list:\n",
    "    train_arr = np.load(train)\n",
    "    test_arr = np.load(test)\n",
    "    data = np.vstack((train_arr, test_arr))\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(f\"{name} {'#'*30}\")\n",
    "    \n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = 40, 1 * data.shape[1]\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "    \n",
    "    fig, axes = plt.subplots(data.shape[1])\n",
    "    axes[0].set_title(name)\n",
    "\n",
    "    for idx, d in enumerate(data.T):\n",
    "        axes[idx].plot(d)\n",
    "        \n",
    "    labels = label_df[label_df[\"chan_id\"] == name][\"anomaly_sequences\"].values[0]\n",
    "    for start, end in labels:\n",
    "        start += len(train_arr)\n",
    "        end += len(train_arr)\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.axvspan(start, end, alpha=0.5, color='red')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "\n",
    "\n",
    "    axes[-1].get_xaxis().set_visible(True)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb320c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreCounter:\n",
    "    def __init__(self, delay: int=None):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.tn = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "        self.delay = delay\n",
    "        \n",
    "        self.point_anom_total = 0\n",
    "        self.point_anom_correct = 0\n",
    "        \n",
    "        self.contextual_anom_total = 0\n",
    "        self.contextual_anom_correct = 0\n",
    "        \n",
    "    def process(self, preds, labels, types):\n",
    "        preds = preds.copy()\n",
    "        labels = labels.copy()\n",
    "        \n",
    "        ground_truth_ones = np.where(labels == 1)[0]\n",
    "        pred_ones = np.where(preds == 1)[0]\n",
    "        ranges = self._consecutive(ground_truth_ones)\n",
    "        \n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        \n",
    "        for idx, r in enumerate(ranges):\n",
    "            intersect = np.intersect1d(r, pred_ones, assume_unique=True)\n",
    "            # if alert delay more than 100 timesteps, count that as bad!\n",
    "            \n",
    "            if types[idx] == \"point\": self.point_anom_total += 1\n",
    "            else: self.contextual_anom_total += 1\n",
    "               \n",
    "            if intersect.size != 0:\n",
    "                cond = intersect[0] < r[0] + self.delay if self.delay is not None else True\n",
    "                if cond:\n",
    "                    tp += r.size\n",
    "                    if types[idx] == \"point\": self.point_anom_correct += 1\n",
    "                    else: self.contextual_anom_correct += 1\n",
    "                else:\n",
    "                    fn += r.size\n",
    " \n",
    "                preds[intersect] = 0\n",
    "                pred_ones = np.where(preds == 1)[0]\n",
    "            else:\n",
    "                fn += r.size\n",
    "            \n",
    "        fp += pred_ones.size\n",
    "        tn += preds.size - tp - fp - fn\n",
    "        \n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.tn += tn\n",
    "        self.fn += fn\n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "        \n",
    "    def _consecutive(self, data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def tpr(self):\n",
    "        return self.tp/(self.fn+self.tp)\n",
    "    \n",
    "    @property\n",
    "    def fpr(self):\n",
    "        return self.fp/(self.tn+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def tnr(self):\n",
    "        return self.tn/(self.tn+self.fp)\n",
    "        \n",
    "    @property\n",
    "    def fnr(self):\n",
    "        return self.fn/(self.fn+self.tp)\n",
    "        \n",
    "    @property\n",
    "    def precision(self):\n",
    "        if self.tp+self.fp == 0: return 0\n",
    "        return self.tp/(self.tp+self.fp)\n",
    "    \n",
    "    @property\n",
    "    def recall(self):\n",
    "        if self.tp+self.fn == 0: return 0\n",
    "        return self.tp/(self.tp+self.fn)\n",
    "    \n",
    "    @property\n",
    "    def f1(self):\n",
    "        if self.recall + self.precision == 0: return 0\n",
    "        return (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "    \n",
    "    @property\n",
    "    def point_correct(self):\n",
    "        if self.point_anom_total == 0: return 0\n",
    "        return self.point_anom_correct/self.point_anom_total\n",
    "    \n",
    "    @property\n",
    "    def contextual_correct(self):\n",
    "        if self.contextual_anom_total == 0: return 0\n",
    "        return self.contextual_anom_correct/self.contextual_anom_total\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{scorer.tp}, {scorer.fp}, {scorer.tn}, {scorer.fn}, {scorer.tpr}, {scorer.fpr}, {scorer.tnr}, {scorer.fnr}, {scorer.precision}, {scorer.recall}, {scorer.f1}, {scorer.point_correct}, {scorer.contextual_correct}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d18770",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.zeros(20), np.full(20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.append(np.zeros(30), np.full(10, 1))\n",
    "pred[10] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 1\n",
    "tn = 19\n",
    "fn = 20\n",
    "\n",
    "scorer = ScoreCounter(delay=5)\n",
    "scorer.process(pred, labels, [\"point\"])\n",
    "\n",
    "assert tp == scorer.tp\n",
    "assert fp == scorer.fp\n",
    "assert tn == scorer.tn\n",
    "assert fn == scorer.fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d45fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea17a73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric-1 Tuned Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f5b1f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"../results/multivar-nasa/unsup1tuned/\"\n",
    "DELAY = 50\n",
    "optuna.logging.set_verbosity(optuna.logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685c151",
   "metadata": {},
   "source": [
    "#### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25317a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_wo_label(data, preds):\n",
    "    \"\"\"\n",
    "    Returns a few quantitative metrics for us to use for evaluation when labels\n",
    "    are not provided.\n",
    "    Parameters\n",
    "    -----------\n",
    "    df: pd.Dataframe\n",
    "        the dataframe with 'timestamp', 'value' and 'predict' columns\n",
    "        where 'predict' is 1 for those predicted as anomalous and 0 otherwise.\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple\n",
    "        (number of anomalies,\n",
    "        % of anomnalies,\n",
    "        avg. distance between mean and all anomalies (yaxis),\n",
    "        avg. time distance between consecutive anomalies,\n",
    "        avg. cycle distance between consecutive anomalies,\n",
    "        maximum range between non anomaly points (yaxis)\n",
    "        )\n",
    "    \"\"\"\n",
    "    num_anomalies = preds.sum()\n",
    "    percent_anomalies = num_anomalies/len(preds)\n",
    "\n",
    "    mean_val = data.mean(axis=0)\n",
    "\n",
    "    pred_anomalies = data[preds == 1]\n",
    "    pred_non_anomalies = data[preds == 0]\n",
    "\n",
    "    avg_anom_dist_from_mean_val = np.linalg.norm(pred_anomalies - mean_val, axis=-1).mean()\n",
    "    avg_cycles_delta_between_anomalies = np.diff(np.where(preds==1)[0]).mean()\n",
    "    max_range_non_anomalies = (np.abs(pred_non_anomalies.sum().max() - pred_non_anomalies.sum().min())).mean() \n",
    "\n",
    "    return (num_anomalies,\n",
    "            percent_anomalies,\n",
    "            avg_anom_dist_from_mean_val,\n",
    "            avg_cycles_delta_between_anomalies,\n",
    "            max_range_non_anomalies)\n",
    "\n",
    "\n",
    "def compute_objective(data, preds):\n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    (num_anomalies, percent_anomalies, avg_anom_dist_from_mean_val,\n",
    "     avg_cycles_delta_between_anomalies, max_range_non_anomalies) = _eval_wo_label(data, preds)\n",
    "\n",
    "    obj = 1e4 * percent_anomalies + max_range_non_anomalies - avg_cycles_delta_between_anomalies\n",
    "    \n",
    "    # If nan, should return number in case it always gives nan\n",
    "    if np.isnan(obj) or np.isinf(obj):\n",
    "        obj = 1e10\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f8f8e",
   "metadata": {},
   "source": [
    "### Single-variate Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77056ce3",
   "metadata": {},
   "source": [
    "#### MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = MovingAverageModel(window)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for series in test_set.T:\n",
    "        score = np.abs(model.get_scores(series)[window:])\n",
    "        scores.append(score)\n",
    "\n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(score, q, contam)\n",
    "\n",
    "        pred = score.copy()\n",
    "        pred[pred <= thres] = 0\n",
    "        pred[pred > thres] = 1\n",
    "\n",
    "        preds.append(pred)\n",
    "\n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(scores, axis=1)\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Moving Average Model\n",
    "model_name = \"ma\"\n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    \n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "   \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-1, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        window = params[\"window\"]\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "        \n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=150)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    window = params[\"window\"]\n",
    "\n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ce4a6",
   "metadata": {},
   "source": [
    "#### Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    thres = params[\"thres\"]\n",
    "    \n",
    "    model = QuantileModel(window, thres)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for series in test_set.T:\n",
    "        score = model.get_scores(series)[window:]\n",
    "        scores.append(score)\n",
    "        preds.append(score)\n",
    "    \n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(scores, axis=1)\n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"quantile\"\n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "done = [\"P-1\", \"T-13\"]\n",
    "for train, test in file_list:\n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    if name in done: continue\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"thres\": trial.suggest_float(\"thres\", 0.9, 0.999),\n",
    "        }\n",
    "\n",
    "        test_extend = np.concatenate((train[-params[\"window\"]:], test))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "            \n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"thres\": study.best_params[\"thres\"],\n",
    "    }\n",
    "\n",
    "    test_extend = np.concatenate((train[-params[\"window\"]:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee80f9d",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c792a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(train_set, test_set, params):\n",
    "    window = params[\"window\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = RegressionModel(window)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for idx, series in enumerate(test_set.T):\n",
    "        model = RegressionModel(window)\n",
    "        model.fit(train_set.T[idx])\n",
    "\n",
    "        score = model.get_scores(series)[0].flatten()\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Get threshold (Not needed for Quantile)\n",
    "        thres = pot(score, q, contam)\n",
    "\n",
    "        pred = score.copy()\n",
    "        pred[pred <= thres] = 0\n",
    "        pred[pred > thres] = 1\n",
    "\n",
    "        preds.append(pred)\n",
    "        \n",
    "    scores = np.array(scores).T\n",
    "    preds = np.any(preds, axis=0)\n",
    "    \n",
    "    \n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ada054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"regression\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 1, 100),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-1, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        window = params[\"window\"]\n",
    "        test_extend = np.concatenate((train[-window:], test))\n",
    "        preds, _ = train_and_score(train, test_extend, params)\n",
    "\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "\n",
    "    window = params[\"window\"]\n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    preds, scores = train_and_score(train, test_extend, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784b0b9",
   "metadata": {},
   "source": [
    "### Multivariate Native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ef93c",
   "metadata": {},
   "source": [
    "#### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train_set, test_set, params):\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    model = IsolationForestModel()\n",
    "    model.fit(train_set)\n",
    "    score = model.get_scores(test_set).flatten()\n",
    "    \n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88423c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"iforest\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for train, test in file_list:\n",
    "    \n",
    "    # Prepare Data\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    i = 0\n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        \n",
    "        return compute_objective(test, preds)\n",
    "        \n",
    "        \n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=150)\n",
    "    \n",
    "    # Get Tune Results\n",
    "    params = {\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "    \n",
    "    # save\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294434b",
   "metadata": {},
   "source": [
    "#### NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = NBEATSModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df449e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"nbeats\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        print(compute_objective(test, preds))\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ac7b2",
   "metadata": {},
   "source": [
    "#### NHiTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc9eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = NHiTSModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dd21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"nhits\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ffdb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RNN-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eddfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = RNNModel(window, n_steps, use_gpu=True, rnn_model=\"GRU\")\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc823f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"rnn_gru\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87ffcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = TCNModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f800a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"tcn\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    tune_len = label_df.loc[label_df[\"chan_id\"]==name][\"anomaly_sequences\"].values[0][0][1] + 1\n",
    "    tune_data = test[:tune_len]\n",
    "    tune_labels = labels[:tune_len]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2ab81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c26698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also \"realigns\" test data\n",
    "def train_and_score(train, test, params):\n",
    "    window = params[\"window\"]\n",
    "    n_steps = params[\"n_steps\"]\n",
    "    q = params[\"q\"]\n",
    "    contam = params[\"contam\"]\n",
    "    \n",
    "    test_extend = np.concatenate((train[-window:], test))\n",
    "    model = TransformerModel(window, n_steps, use_gpu=True)\n",
    "    model.fit(train)\n",
    "    model.fit(train)\n",
    "    \n",
    "    score = model.get_scores(test_extend)[0].flatten()\n",
    "    res = model.get_scores(test_extend)[1]\n",
    "    \n",
    "    # Get threshold (Not needed for Quantile)\n",
    "    thres = pot(score, q, contam)\n",
    "\n",
    "    pred = score.copy()\n",
    "    pred[pred <= thres] = 0\n",
    "    pred[pred > thres] = 1\n",
    "    \n",
    "    return pred, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eee2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "\n",
    "model_name = \"transformer\"\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "\n",
    "for train, test in file_list:\n",
    "    name = train.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    train = np.load(train)\n",
    "    test = np.load(test)\n",
    "    labels = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "        \n",
    "    def objective(trial):\n",
    "        s = ScoreCounter(delay=DELAY)\n",
    "\n",
    "        params = {\n",
    "            \"window\": trial.suggest_int(\"window\", 11, 100),\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
    "            \"q\": trial.suggest_float(\"q\", 1e-7, 1e-2, log=True),\n",
    "            \"contam\": trial.suggest_float(\"contam\", 0.90, 0.999)\n",
    "        }\n",
    "\n",
    "        preds, _ = train_and_score(train, test, params)\n",
    "        return compute_objective(test, preds)\n",
    "\n",
    "    # Tune\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=35)\n",
    "    params = {\n",
    "        \"window\": study.best_params[\"window\"],\n",
    "        \"n_steps\": study.best_params[\"n_steps\"],\n",
    "        \"q\": study.best_params[\"q\"],\n",
    "        \"contam\": study.best_params[\"contam\"]\n",
    "    }\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "\n",
    "    save = f\"{SAVE_DIR}{model_name}/{name}\"\n",
    "    os.makedirs(SAVE_DIR+model_name, exist_ok=True)\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"transformer\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"./results/multivar-nasa/f1tuned-1anom/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"preds\" not in i]\n",
    "\n",
    "params_file = {}\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    with open(SAVE_DIR+method+f, 'r') as file:\n",
    "        params = file.readline()\n",
    "        \n",
    "    params = params.strip()\n",
    "    params = params.replace(\"# \", \"\")\n",
    "    params = params.replace(\"'\", '\"')\n",
    "    params = json.loads(params)\n",
    "    \n",
    "    params_file.update({name: params})\n",
    "\n",
    "print(params)\n",
    "    \n",
    "#######################\n",
    "print(\"here\")\n",
    "#######################\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    test_path = file_list[idx][1]\n",
    "    test = np.load(test_path)\n",
    "    \n",
    "    train_path = file_list[idx][0]\n",
    "    train = np.load(train_path)\n",
    "    \n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    params = params_file[name]\n",
    "    \n",
    "    preds, scores = train_and_score(train, test, params)\n",
    "    save = f\"{SAVE_DIR}{method}/{name}\"\n",
    "    np.savetxt(save+\"-scores.txt\", scores, header=str(params))\n",
    "    np.savetxt(save+\"-preds.txt\", preds, header=str(params))\n",
    "\n",
    "    scorer.process(preds, labels, types)\n",
    "\n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ae29d",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81437958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = \"transformer\"\n",
    "\n",
    "#######################\n",
    "SAVE_DIR = \"../results/multivar-nasa/unsup1tuned/\"\n",
    "method += \"/\"\n",
    "l = [i for i in get_files_from_path(SAVE_DIR+method) if \"scores\" not in i]\n",
    "scorer = ScoreCounter(delay=DELAY)\n",
    "for f in l:\n",
    "    name = \"-\".join(f.split(\"-\")[:2])\n",
    "    idx = [name in i[0] for i in file_list].index(True)\n",
    "    \n",
    "    path = file_list[idx][1]\n",
    "    data = np.load(path)\n",
    "    label = data_labels[name]\n",
    "    types = label_df.loc[label_df[\"chan_id\"]==name][\"class\"].values[0]\n",
    "    \n",
    "    preds = np.loadtxt(SAVE_DIR+method+f)\n",
    "    \n",
    "    scorer.process(preds, label, types)\n",
    "    \n",
    "print(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13aa2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
